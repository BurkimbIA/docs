{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#welcome","title":"Welcome","text":"<p>Welcome to the technical documentation of the BurkimBbA project. This open-source project is not only about translation but also about building and contributing to the development of local AI for everyone. We aim to collect, process, and translate texts, audio, and videos in the Moor\u00e9 language to create efficient and accessible AI-driven solutions.</p>"},{"location":"#documentation-structure","title":"Documentation Structure","text":"<p>Our documentation follows the golden tech structure proposed by Divio, which organizes content into four main categories:</p> <p></p> <ol> <li>Tutorials: Step-by-step guides to help you get started with the project.</li> <li>How-to guides: Practical instructions to complete specific tasks.</li> <li>Explanations: Insights into key concepts and technical decisions.</li> <li>References: Detailed documentation of APIs, file formats, and configurations.</li> </ol> <p>\ud83d\udcd6 Learn More</p> <ul> <li> <p>Divio Documentation - Explanations</p> </li> <li> <p>Divio Documentation - How-to Guides</p> </li> <li> <p>Divio Documentation - Tutorials</p> </li> </ul>"},{"location":"#how-to-contribute","title":"How to Contribute","text":"<ol> <li>Read this page and learn how to classify your documentation properly.</li> <li>Create a branch and submit a Pull Request.</li> <li>At least two reviewers' approvals are required before merging.</li> <li>What to change:</li> <li>Create a file in the appropriate category (e.g., <code>docs/explanations/my_article.md</code>).</li> <li>Update the <code>SUMMARY.md</code> file to include your new file path.</li> </ol> <p>We hope this documentation helps you understand, use, and contribute to the project effectively. Together, we are building AI solutions that empower local communities. Happy exploring! \ud83d\ude80</p>"},{"location":"explanations/deploiement_llm_serverless/","title":"deploiement llm sans cluster","text":"<p>Notre equipe Burkimbia doit servir des modeles pour des langues africaines sans exploser un budget limite a vingt euros par mois. Ce billet raconte comment nous avons arbitre entre plusieurs fournisseurs et pourquoi le mode serverless reste aujourd'hui notre garde fou. Pas de pas a pas ici, uniquement les choix strategiques et leurs impacts.</p>"},{"location":"explanations/deploiement_llm_serverless/#nos-contraintes-terrain","title":"nos contraintes terrain","text":"<ul> <li>usage intermittent: environ trois mille requetes mensuelles, chacune autour de 8.5 secondes de calcul GPU.</li> <li>public fragile: trente pour cent des requetes sont offertes pour des apprenantes et apprenants sans moyens.</li> <li>equipe reduite: pas de temps pour maintenir un cluster ECS ou EKS.</li> <li>transparence: tout cout doit etre justifiable pour nos financeurs.</li> </ul> <p>Ces contraintes rendent toute facturation continue impraticable. Louer un T4 en permanence chez AWS ou Azure revient a plus de 360 dollars mensuels. Avec le mode serverless, seules les secondes effectives sont facturees.</p>"},{"location":"explanations/deploiement_llm_serverless/#comparatif-rapide-des-plateformes","title":"comparatif rapide des plateformes","text":"plateforme gpu modele de facturation cout mensuel estime cout pour 1000 requetes aws ec2 g4dn.xlarge 720 heures payees 378.72 usd 126.24 usd azure vm nc4as_t4_v3 720 heures payees 378.72 usd 126.24 usd hugging face endpoints t4 720 heures payees 360.00 usd 120.00 usd runpod flex rtx a6000 a la seconde 8.67 usd 2.89 usd <p>Les chiffres proviennent des grilles officielles d'octobre 2025. Seul RunPod Flex respecte notre budget tout en offrant une A6000 pour les modeles de 7b quantifies.</p>"},{"location":"explanations/deploiement_llm_serverless/#lessons-retenues-apres-un-incident-couteux","title":"lessons retenues apres un incident couteux","text":"<p>Un oubli de execution timeout a prolonge un worker RunPod pendant plusieurs jours. Plus de cinq mille requetes de ping ont maintenu la session ouverte et genere des frais serverless importants. Depuis, nous appliquons trois regles:</p> <ol> <li>fixer idle_timeout et execution_timeout pour chaque endpoint.</li> <li>limiter max_workers a 1 tant que la charge reste faible.</li> <li>monitorer la frequence de ping via nos journaux et couper tout traffic suspect.</li> </ol>"},{"location":"explanations/deploiement_llm_serverless/#images-docker-minimalistes","title":"images docker minimalistes","text":"<p>Nos premieres images PyTorch depassaient neuf gigaoctets. Cela rallongeait chaque cold start. Nous sommes passes a une base Nvidia CUDA puis installons seulement les builds necessaires (Torch, BitsAndBytes, SentencePiece). Chaque image contient directement le modele quantifie pour eviter un telechargement au demarrage. Avec cette approche, les cold start descendent sous quinze secondes sur A6000 et les surprises de facturation disparaissent.</p>"},{"location":"explanations/deploiement_llm_serverless/#quantification-et-matrice-gpu","title":"quantification et matrice gpu","text":"<p>Nous ne lancons aucun modele non quantifie sur RunPod. Mistral 7b passe de vingt gigaoctets en FP16 a environ quatre gigaoctets en INT4. Cela rend l'image plus legere et nous autorise a rester sur A6000 ou meme A4000 pour des charges plus modestes. La regle interne: quantifier d'abord, tester ensuite le plus petit GPU disponible, ne monter en gamme que si la latence explose.</p>"},{"location":"explanations/deploiement_llm_serverless/#suivi-budgetaire-en-temps-reel","title":"suivi budgetaire en temps reel","text":"<p>Trois indicateurs sont exposes dans notre tableau de bord Metabase:</p> <ul> <li>cold_start_seconds: objectif sous quinze secondes.</li> <li>gpu_seconds_total: converti en cout temps reel en euros.</li> <li>cost_per_1000_requests: alerte quand on depasse 3 usd.</li> </ul> <p>Nous declenchons une alerte Slack quand quatre-vingts pour cent du budget mensuel sont consommes. La transparence vis-a-vis des partenaires rend plus faciles les demandes de financement ponctuel.</p>"},{"location":"explanations/deploiement_llm_serverless/#stack-hybride-recommandee","title":"stack hybride recommandee","text":"<ul> <li>production: endpoints RunPod Serverless en RTX A6000 avec timeouts serre.</li> <li>prototypes publics: Hugging Face Spaces ZeroGPU a dix dollars pour heberger des demos Gradio.</li> <li>experiments internes: instances locales consumer GPU pour iterer sur les quantifications.</li> </ul> <p>Cette combinaison maintient les couts sous vingt euros par mois tout en gardant une vitrine publique simple a partager.</p>"},{"location":"explanations/deploiement_llm_serverless/#checklist-strategique-avant-tout-nouveau-modele","title":"checklist strategique avant tout nouveau modele","text":"<ol> <li>verifier la taille en FP16 puis en INT4 et mettre a jour le plan de build Docker.</li> <li>recalculer le cout par mille requetes avec les nouvelles latences mesurees.</li> <li>mettre a jour idle_timeout, execution_timeout et les limites max_workers.</li> <li>ajouter le modele au tableau de bord couts et tester les alertes.</li> <li>documenter l'impact pour les financeurs avant le lancement public.</li> </ol> <p>Rester frugal n'est pas un choix ideologique mais une condition de survie pour une organisation a but non lucratif. Cette fiche conceptuelle doit guider toute nouvelle initiative autour des LLM chez Burkimbia.</p>"},{"location":"explanations/machine_translation/","title":"machine translation stack","text":"<p>Cette note expose notre approche pour un systeme de traduction couvrant le francais et le moore. L'objectif est de rester opensource, modulaire et realiste pour une equipe reduite.</p>"},{"location":"explanations/machine_translation/#objectifs-du-systeme","title":"objectifs du systeme","text":"<ul> <li>text-to-text: traduction bidirectionnelle entre francais et moore.</li> <li>speech-to-text: transcription des audios francais et moore.</li> <li>text-to-speech: generation de parole naturelle en moore.</li> </ul> <p>Les ressources etiquetees restent rares, d'ou l'obligation de miser sur le transfert d'apprentissage et la quantification.</p>"},{"location":"explanations/machine_translation/#text-to-text","title":"text to text","text":""},{"location":"explanations/machine_translation/#modeles-a-privilegier","title":"modeles a privilegier","text":"<ul> <li><code>nllb-200</code> pour la couverture des langues peu dotees.</li> <li><code>mbart-50</code> comme base polyvalente facile a affiner.</li> <li><code>m2m-100</code> pour eviter un pivot par l'anglais.</li> <li><code>marianmt</code> lorsque l'on cherche un modele compacte.</li> <li><code>mistral 7b</code> ou autres <code>llm</code> legers apres quantification <code>int4</code>.</li> </ul>"},{"location":"explanations/machine_translation/#ameliorations-cibles","title":"ameliorations cibles","text":"<ul> <li>constitution d'un corpus parallele francais-moore et fine tuning specifique.</li> <li>back translation pour augmenter artificiellement la portion moore.</li> <li>couches <code>adapter</code> pour personnaliser sans toucher a tous les poids.</li> <li>auto etiquetage via notre pipeline <code>stt</code> pour produire du parallele low-cost.</li> </ul>"},{"location":"explanations/machine_translation/#mesures-de-qualite","title":"mesures de qualite","text":"<ul> <li><code>bleu</code> et <code>chrf++</code> pour suivre les progres.</li> <li>evaluation humaine par nos locuteurs natifs.</li> <li>suivi de la latence pour garantir une inference acceptable sur <code>a4000</code> ou <code>a6000</code>.</li> </ul>"},{"location":"explanations/machine_translation/#speech-to-text","title":"speech to text","text":""},{"location":"explanations/machine_translation/#options-de-modele","title":"options de modele","text":"<ul> <li><code>whisper</code> adapte au moore apres un leger affinement.</li> <li><code>wav2vec 2.0</code> pour profiter de l'apprentissage auto supervise.</li> <li>architectures <code>conformer</code> lorsque l'on veut pousser la precision.</li> </ul>"},{"location":"explanations/machine_translation/#collecte-et-augmentation","title":"collecte et augmentation","text":"<ul> <li>campagnes de crowdsourcing dans les communautes partenaires.</li> <li>generation synthetique avec modification de vitesse, hauteur ou bruit.</li> <li>augmentation phonemique pour couvrir les variantes dialectales.</li> </ul>"},{"location":"explanations/machine_translation/#suivi-de-performance","title":"suivi de performance","text":"<ul> <li><code>wer</code> comme indicateur principal.</li> <li><code>per</code> pour surveiller les confusions phonemiques.</li> <li><code>rtf</code> pour garantir une inference proche du temps reel.</li> </ul>"},{"location":"explanations/machine_translation/#text-to-speech","title":"text to speech","text":""},{"location":"explanations/machine_translation/#modeles-candidats","title":"modeles candidats","text":"<ul> <li><code>tacotron 2</code> pour la qualite vocale.</li> <li><code>fastspeech 2</code> pour la rapidite.</li> <li><code>vits</code> quand on veut un pipeline de bout en bout.</li> </ul>"},{"location":"explanations/machine_translation/#leviers-de-progression","title":"leviers de progression","text":"<ul> <li>adaptation par locuteur a partir de corpus moore collectes localement.</li> <li>modelisation de la prosodie pour un rendu expressif.</li> <li>augmentation avec perturbations audio et representations phonemiques.</li> </ul>"},{"location":"explanations/machine_translation/#metriques-clefs","title":"metriques clefs","text":"<ul> <li><code>mos</code> obtenu via ecoutes internes.</li> <li><code>mcd</code> pour suivre la qualite spectrale.</li> <li><code>cer</code> afin d'assurer l'intelligibilite.</li> </ul>"},{"location":"explanations/machine_translation/#integration-pipeline","title":"integration pipeline","text":""},{"location":"explanations/machine_translation/#traduction-ecrite","title":"traduction ecrite","text":"<pre><code>graph LR;\n    A[texte source] --&gt; B[normalisation]\n    B --&gt; C[selection du modele]\n    C --&gt; D[post-traitement]\n    D --&gt; E[texte traduit]</code></pre>"},{"location":"explanations/machine_translation/#chaine-voix","title":"chaine voix","text":"<pre><code>graph LR;\n    A[audio] --&gt; B[pretraitement]\n    B --&gt; C[modele stt]\n    C --&gt; D[modele de langage]\n    D --&gt; E[post-traitement]\n    E --&gt; F[transcription]</code></pre>"},{"location":"explanations/machine_translation/#defis-a-surveiller","title":"defis a surveiller","text":"<ul> <li>penurie de donnees moore etiquetees.</li> <li>variabilite dialectale entre regions.</li> <li>contraintes de deploiement sur du materiel limite.</li> <li>besoin futur de support multimodal (texte, audio, visuel).</li> </ul>"},{"location":"how-to/data_storage/","title":"Accessing and Storing Data in our stack","text":"<p>This guide provides a technical walkthrough for interacting with S3-compatible object storage, using Flio S3 as a practical example.  The principles and code snippets are generally applicable to other S3-compatible services like AWS S3.</p>"},{"location":"how-to/data_storage/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have the following:</p> <ul> <li>An S3-Compatible Storage Account: You'll need an account with an S3-compatible storage provider. This guide uses Flio S3 as an example, but you can adapt it for other providers.</li> <li>Access Credentials: Obtain your <code>AWS_ACCESS_KEY_ID</code>, <code>AWS_SECRET_ACCESS_KEY</code>, and the <code>AWS_ENDPOINT_URL_S3</code> for your storage provider.</li> <li>Python Installed: Ensure you have Python 3 installed on your system.</li> <li>Required Python Libraries: Install the necessary libraries using pip:     <pre><code>pip install boto3 s3fs python-dotenv pandas datasets\n</code></pre><ul> <li><code>boto3</code>: The AWS SDK for Python.</li> <li><code>s3fs</code>: A library that provides a file-like interface to S3.</li> <li><code>python-dotenv</code>: For loading environment variables from a <code>.env</code> file.</li> <li><code>pandas</code>: For working with data in a tabular format (CSV).</li> <li><code>datasets</code>: For interacting with datasets, particularly from Hugging Face.</li> </ul> </li> </ul>"},{"location":"how-to/data_storage/#1-understanding-s3-buckets","title":"1. Understanding S3 Buckets","text":"<p>Here's the corrected version with improved clarity and grammar:  </p> <p>An S3 bucket is like a top-level folder in S3-compatible storage. It helps you store and organize everything\u2014files, images, videos, datasets, and more. However, unlike regular folders, an S3 bucket is designed for scalability, durability, and remote access.  </p> <p>Each object inside a bucket has a unique key (path), just like files inside folders. But in S3, there are no actual \"subfolders\"\u2014everything is stored in a flat structure, and folder-like organization is achieved through naming conventions (e.g., <code>project/images/photo.jpg</code>).  </p> <p>Our bucket can be viewed directly on the site Flio at Fly Storage. Simply scroll down on the main page, select Tigris Object Storage, and access your data.  </p> <p> </p> <p>Now, let\u2019s navigate to a specific path and explore the stored data:  </p> <p> </p> <p>So, now that we will store our data and files here, how can we push files directly to the bucket or read them?</p>"},{"location":"how-to/data_storage/#2-setting-up-access-credentials-for-secure-interaction","title":"2. Setting Up Access Credentials for Secure Interaction","text":"<p>To allow programmatic access to your S3 bucket, you'll need to configure access credentials. These are crucial for authentication and ensuring secure machine-to-machine communication.</p>"},{"location":"how-to/data_storage/#essential-credentials","title":"Essential Credentials","text":"<ul> <li><code>AWS_ACCESS_KEY_ID</code>: Your public identifier.</li> <li><code>AWS_SECRET_ACCESS_KEY</code>: Your private secret key (keep this secure!).</li> <li><code>AWS_ENDPOINT_URL_S3</code>: The specific service endpoint. For Flio S3, this is <code>https://fly.storage.tigris.dev</code>.</li> </ul>"},{"location":"how-to/data_storage/#secure-storage-with-env-files","title":"Secure Storage with <code>.env</code> Files","text":"<p>It's highly recommended to store your credentials in a <code>.env</code> file in your root folder to prevent accidentally exposing them in your code.</p> <pre><code># .env\nAWS_ACCESS_KEY_ID=your_public_key_here\nAWS_SECRET_ACCESS_KEY=your_secret_key_here\nAWS_ENDPOINT_URL_S3=weareburkima.dev\n</code></pre> <p>You can load these credentials into your Python environment using the <code>python-dotenv</code> library:</p> <pre><code>from dotenv import load_dotenv\nimport os\n\nload_dotenv(\".env\")  # Load variables from .env\n</code></pre>"},{"location":"how-to/data_storage/#3-configuring-your-s3-client","title":"3. Configuring Your S3 Client","text":"<p>You can interact with S3-compatible storage using various libraries. Two popular options in Python are <code>boto3</code> (the AWS SDK) and <code>s3fs</code> (which provides a file system-like interface).</p>"},{"location":"how-to/data_storage/#using-boto3-for-direct-api-calls","title":"Using <code>boto3</code> for Direct API Calls","text":"<p><code>boto3</code> offers a comprehensive interface for interacting with S3.</p> <pre><code>import boto3\n\naccess_key = os.getenv(\"AWS_ACCESS_KEY_ID\")\nsecret_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\nendpoint_url = os.getenv(\"AWS_ENDPOINT_URL_S3\")\n\ns3_client = boto3.client(\n    \"s3\",\n    aws_access_key_id=access_key,\n    aws_secret_access_key=secret_key,\n    endpoint_url=endpoint_url,\n)\n</code></pre>"},{"location":"how-to/data_storage/#using-s3fs-for-a-file-system-interface","title":"Using <code>s3fs</code> for a File System Interface","text":"<p><code>s3fs</code> allows you to interact with your S3 bucket as if it were a local file system, which can be convenient for certain operations.</p> <pre><code>import s3fs\n\naccess_key = os.getenv(\"AWS_ACCESS_KEY_ID\")\nsecret_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\nendpoint_url = os.getenv(\"AWS_ENDPOINT_URL_S3\")\n\nfs = s3fs.S3FileSystem(\n    key=access_key,\n    secret=secret_key,\n    endpoint_url=endpoint_url\n)\n</code></pre>"},{"location":"how-to/data_storage/#4-listing-files-within-a-bucket","title":"4. Listing Files Within a Bucket","text":"<p>You'll often need to see what files are stored in your bucket. Here's how to do it with both <code>s3fs</code> and <code>boto3</code>.</p>"},{"location":"how-to/data_storage/#listing-with-s3fs","title":"Listing with <code>s3fs</code>","text":"<pre><code>BUCKET_NAME = \"burkimbia\"\nprefix = \"optional/folder/prefix/\"  # Optional: Filter files within a specific folder\n\ntry:\n    files = fs.ls(f\"{BUCKET_NAME}/{prefix}\")\n    print(f\"Files in s3://{BUCKET_NAME}/{prefix}:\")\n    for file in files:\n        print(file)\nexcept Exception as e:\n    print(f\"Error listing files: {e}\")\n</code></pre>"},{"location":"how-to/data_storage/#listing-with-boto3","title":"Listing with <code>boto3</code>","text":"<pre><code>def list_s3_files(s3_client, bucket_name, prefix=\"\"):\n    files = []\n    paginator = s3_client.get_paginator(\"list_objects_v2\")\n    try:\n        for page in paginator.paginate(Bucket=bucket_name, Prefix=prefix):\n            for obj in page.get(\"Contents\", []):\n                files.append(obj[\"Key\"])\n        return files\n    except Exception as e:\n        print(f\"Error listing files: {e}\")\n        return []\n\nBUCKET_NAME = \"burkimbia\"\nprefix = \"sandbox/raw_data/\"\n\nfile_list = list_s3_files(s3_client, BUCKET_NAME, prefix)\nprint(f\"Files in s3://{BUCKET_NAME}/{prefix}:\")\nfor file in file_list:\n    print(file)\n</code></pre>"},{"location":"how-to/data_storage/#5-uploading-files-to-your-bucket","title":"5. Uploading Files to Your Bucket","text":"<p>To store data, you'll need to upload files to your S3 bucket.</p> <pre><code>def upload_file_to_s3(s3_client, local_path, bucket_name, s3_key):\n    if not os.path.exists(local_path):\n        raise FileNotFoundError(f\"Local file not found: {local_path}\")\n    try:\n        s3_client.upload_file(local_path, bucket_name, s3_key)\n        print(f\"Uploaded '{local_path}' to s3://{bucket_name}/{s3_key}\")\n    except Exception as e:\n        print(f\"Error uploading '{local_path}': {e}\")\n\nBUCKET_NAME = \"burkimbia\"\nlocal_file_path = \"path/to/your/local_file.txt\"\ns3_object_key = \"sandbox/cooked_data/uploaded_file.txt\"\n\nupload_file_to_s3(s3_client, local_file_path, BUCKET_NAME, s3_object_key)\n</code></pre>"},{"location":"how-to/data_storage/#6-downloading-files-from-your-bucket","title":"6. Downloading Files from Your Bucket","text":"<p>Retrieving data from your S3 bucket is done by downloading files.</p> <pre><code>def download_file_from_s3(s3_client, bucket_name, s3_key, local_path):\n    os.makedirs(os.path.dirname(local_path), exist_ok=True)\n    try:\n        s3_client.download_file(bucket_name, s3_key, local_path)\n        print(f\"Downloaded 's3://{bucket_name}/{s3_key}' to '{local_path}'\")\n    except Exception as e:\n        print(f\"Error downloading 's3://{bucket_name}/{s3_key}': {e}\")\n\nBUCKET_NAME = \"burkimbia\"\ns3_object_key = \"path/to/your/file/in/s3.txt\"\nlocal_file_path = \"path/to/save/downloaded_file.txt\"\n\ndownload_file_from_s3(s3_client, BUCKET_NAME, s3_object_key, local_file_path)\n</code></pre>"},{"location":"how-to/data_storage/#7-reading-and-writing-data-directly","title":"7. Reading and Writing Data Directly","text":"<p>Often, you'll want to read data directly from S3 into your applications without downloading it first, or write data directly to S3.</p>"},{"location":"how-to/data_storage/#reading-json-with-s3fs","title":"Reading JSON with <code>s3fs</code>","text":"<pre><code>import json\n\nBUCKET_NAME = \"burkimbia\"\ns3_json_path = f\"{BUCKET_NAME}/sandbox/cooked_data/data.json\"\n\ntry:\n    with fs.open(s3_json_path) as f:\n        data = json.load(f)\n        print(\"JSON Data:\", data)\nexcept Exception as e:\n    print(f\"Error reading JSON from {s3_json_path}: {e}\")\n</code></pre>"},{"location":"how-to/data_storage/#reading-and-writing-csv-data-with-pandas","title":"Reading and Writing CSV Data with Pandas","text":"<p>The <code>pandas</code> library can directly read and write CSV files from and to S3 using <code>s3fs</code> under the hood.</p> <pre><code>import pandas as pd\n\nBUCKET_NAME = \"burkimbia\"\ns3_csv_path = f\"{BUCKET_NAME}/sandbox/cooked_data/data.csv\"\ns3_parquet_path = f\"{BUCKET_NAME}/sandbox/cooked_data/data.parquet\"\n\n# Reading CSV\ntry:\n    df = pd.read_csv(\n        s3_csv_path,\n        storage_options={\n            \"key\": access_key,\n            \"secret\": secret_key,\n            \"client_kwargs\": {\"endpoint_url\": endpoint_url}\n        }\n    )\n    print(\"CSV Data:\")\n    print(df.head())\nexcept Exception as e:\n    print(f\"Error reading CSV from {s3_csv_path}: {e}\")\n\n# Writing Parquet\ntry:\n    df.to_parquet(\n        s3_parquet_path,\n        storage_options={\n            \"key\": access_key,\n            \"secret\": secret_key,\n            \"client_kwargs\": {\"endpoint_url\": endpoint_url}\n        }\n    )\n    print(f\"Data written to {s3_parquet_path}\")\nexcept Exception as e:\n    print(f\"Error writing Parquet to {s3_parquet_path}: {e}\")\n</code></pre>"},{"location":"how-to/data_storage/#integration-with-hugging-face-datasets","title":"Integration with Hugging Face Datasets","text":"<p>The <code>datasets</code> library from Hugging Face can also interact with S3 for storing and loading datasets.</p> <pre><code>from datasets import Dataset\n\nBUCKET_NAME = \"your-bucket-name\"\ns3_dataset_path = f\"s3://{BUCKET_NAME}/golden/text_to_text\"\n\ntry:\n    # Assuming you have a Pandas DataFrame 'df'\n    dataset = Dataset.from_pandas(df)\n    dataset.save_to_disk(\n        s3_dataset_path,\n        storage_options={\n            \"key\": access_key,\n            \"secret\": secret_key,\n            \"client_kwargs\": {\"endpoint_url\": endpoint_url}\n        }\n    )\n    print(f\"Hugging Face dataset saved to {s3_dataset_path}\")\n\n    # Load dataset from S3\n    loaded_dataset = Dataset.load_from_disk(\n        s3_dataset_path,\n        storage_options={\n            \"key\": access_key,\n            \"secret\": secret_key,\n            \"client_kwargs\": {\"endpoint_url\": endpoint_url}\n        }\n    )\n    print(\"Loaded Hugging Face dataset:\", loaded_dataset)\n\nexcept Exception as e:\n    print(f\"Error interacting with Hugging Face dataset on {s3_dataset_path}: {e}\")\n</code></pre>"},{"location":"references/","title":"documentation style","text":"<p>Cette section sert d'exemple minimal pour tester la navigation <code>references</code>.</p>"},{"location":"tutorials/","title":"Example of tuto wtf i don't know i to name","text":""},{"location":"tutorials/#overview","title":"Overview","text":"<p>Briefly describe what this component or feature does.</p>"},{"location":"tutorials/#prerequisites","title":"Prerequisites","text":"<p>List any prerequisites or dependencies required.</p>"},{"location":"tutorials/#usage","title":"Usage","text":"<p>Show how to use this component with code examples.</p> <pre><code># Example code\nfrom your_project import YourComponent\n\ncomponent = YourComponent(param1=value1)\nresult = component.process(data)\n</code></pre>"},{"location":"tutorials/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>param1</code> str <code>None</code> Description of param1 <code>param2</code> int <code>42</code> Description of param2"},{"location":"tutorials/#return-values","title":"Return Values","text":"<p>Describe what is returned and in what format.</p>"},{"location":"tutorials/#examples","title":"Examples","text":"<p>Provide 1-2 complete examples with expected outputs.</p>"},{"location":"tutorials/#common-issues","title":"Common Issues","text":"<p>List common problems and their solutions.</p>"},{"location":"tutorials/#see-also","title":"See Also","text":"<p>Link to related documentation.</p>"},{"location":"tutorials/runpod_serverless/","title":"runpod serverless llm tutoriel","text":"<p>Ce tutoriel detaille la sequence pour livrer un service <code>llm</code> sur <code>runpod serverless</code>. Le but est d'obtenir un endpoint resilient et peu couteux pour l'association <code>burkimbia</code>.</p>"},{"location":"tutorials/runpod_serverless/#prerequis","title":"prerequis","text":"<ul> <li>une image docker basee sur <code>nvidia cuda</code> contenant votre modele quantifie.</li> <li>un compte <code>runpod</code> avec une cle <code>RUNPOD_API_KEY</code>.</li> <li>un compte <code>hugging face</code> si le modele est prive et necessite <code>HF_TOKEN</code>.</li> <li><code>python 3.10</code> ou plus avec les dependances du script <code>deploy_runpod.py</code>.</li> </ul> <p>Stockez les secrets dans votre shell avant de commencer:</p> <pre><code>export RUNPOD_API_KEY=\"votre_cle\"\nexport HF_TOKEN=\"votre_token\"\nexport DOCKER_USERNAME=\"votre_identifiant\"\n</code></pre>"},{"location":"tutorials/runpod_serverless/#etape-1-construire-une-image-docker-legere","title":"etape 1 construire une image docker legere","text":"<ol> <li>partez d'une base <code>nvidia/cuda:12.1.1-runtime-ubuntu22.04</code>.</li> <li>installez <code>python</code>, <code>pip</code> et uniquement les bibliotheques necessaires (<code>torch</code>, <code>transformers</code>, <code>bitsandbytes</code>).</li> <li>copiez le modele quantifie dans <code>/models</code> pour eviter tout telechargement a chaud.</li> <li>definissez <code>RUNPOD_INIT_TIMEOUT=800</code> et <code>PYTHONUNBUFFERED=1</code> dans le <code>Dockerfile</code>.</li> </ol> <p>Construisez et poussez l'image:</p> <pre><code>docker build -t docker.io/votre_repo/llm-service:latest .\ndocker push docker.io/votre_repo/llm-service:latest\n</code></pre>"},{"location":"tutorials/runpod_serverless/#etape-2-parametrer-runpod-configjson","title":"etape 2 parametrer <code>runpod-config.json</code>","text":"<p>Cr\u00e9ez un fichier a la racine du projet:</p> <pre><code>{\n  \"translation_endpoint\": {\n    \"endpoint_name\": \"burkimbia-translation-service\",\n    \"model_type\": \"translation\",\n    \"image\": \"docker.io/votre_repo/llm-service:latest\",\n    \"gpu_types\": [\"NVIDIA RTX A4000\"],\n    \"idle_timeout\": 5,\n    \"execution_timeout\": 60,\n    \"scaling\": {\n      \"min_workers\": 0,\n      \"max_workers\": 1\n    },\n    \"env\": {\n      \"HF_TOKEN\": \"${HF_TOKEN}\",\n      \"MODEL_PATH\": \"/models/mistral-7b-int4\"\n    }\n  }\n}\n</code></pre> <p>Selectionnez le plus petit <code>gpu_types</code> possible. <code>execution_timeout</code> doit couvrir le temps de reponse maximal observe. Gardez <code>max_workers</code> a <code>1</code> tant que la charge reste modeste.</p>"},{"location":"tutorials/runpod_serverless/#etape-3-lancer-le-script-deploiement-local","title":"etape 3 lancer le script deploiement local","text":"<p>Depuis la racine du depot:</p> <pre><code>python deploy_runpod.py --config runpod-config.json --force\n</code></pre> <p>Le script <code>RunPodDeployer</code> va:</p> <ul> <li>verifier la presence des secrets requis.</li> <li>creer ou recreer le template serverless.</li> <li>appliquer la configuration <code>idle_timeout</code> et <code>execution_timeout</code>.</li> <li>generer <code>runpod_deployed_endpoints.json</code> contenant l'identifiant du service.</li> </ul> <p>Si vous preferez <code>github actions</code>, declenchez le workflow <code>DEPLOY - RunPod AI Services</code> et cochez l'option <code>Force recreate template</code>.</p>"},{"location":"tutorials/runpod_serverless/#etape-4-tester-lendpoint","title":"etape 4 tester l'endpoint","text":"<p>Utilisez le client interne <code>common/runpod_client.py</code>.</p> <pre><code>import os\nfrom common.runpod_client import RunPodClient\n\nclient = RunPodClient(api_token=os.environ[\"RUNPOD_API_KEY\"])\nresult = client.translate(\n    text=\"Bonjour le monde\",\n    service_id=\"ENDPOINT_ID\",\n    src_lang=\"fra_Latn\",\n    tgt_lang=\"moor_Latn\",\n    model_type=\"nllb\"\n)\nprint(result)\n</code></pre> <p>Verifiez que <code>status</code> vaut <code>COMPLETED</code> et que le temps total reste sous votre seuil. Activez <code>verbose=True</code> pour inspecter le <code>cold_start</code> lors de la premiere requete.</p>"},{"location":"tutorials/runpod_serverless/#etape-5-surveiller-et-durcir","title":"etape 5 surveiller et durcir","text":"<ul> <li>surveillez les journaux <code>runpod</code> pour rep\u00e9rer des <code>ping</code> anormaux.</li> <li>configurez une alerte <code>slack</code> quand <code>gpu_seconds_total</code> depasse 80 pour cent du budget mensuel.</li> <li>fixez <code>job_ttl</code> a une valeur courte (par exemple <code>600</code> secondes) pour eviter la retention de jobs termines.</li> <li>mettez a jour l'image docker a chaque changement de modele, puis relancez le workflow.</li> </ul>"},{"location":"tutorials/runpod_serverless/#nettoyage","title":"nettoyage","text":"<p>Quand un endpoint n'est plus necessaire:</p> <pre><code>python deploy_runpod.py --config runpod-config.json --delete translation_endpoint\n</code></pre> <p>Cela supprime l'endpoint et libere le worker. Conservez le <code>template</code> uniquement si vous prevez un redeploiement rapide.</p> <p>En suivant ces etapes, vous obtenez un service <code>llm</code> serverless frugal et reproductible, adapte aux contraintes de <code>burkimbia</code>.</p>"}]}