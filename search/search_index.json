{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#welcome","title":"Welcome","text":"<p>Welcome to the technical documentation of the BurkimBbA project. This open-source project is not only about translation but also about building and contributing to the development of local AI for everyone. We aim to collect, process, and translate texts, audio, and videos in the Moor\u00e9 language to create efficient and accessible AI-driven solutions.</p>"},{"location":"#documentation-structure","title":"Documentation Structure","text":"<p>Our documentation follows the golden tech structure proposed by Divio, which organizes content into four main categories:</p> <p></p> <ol> <li>Tutorials: Step-by-step guides to help you get started with the project.</li> <li>How-to guides: Practical instructions to complete specific tasks.</li> <li>Explanations: Insights into key concepts and technical decisions.</li> <li>References: Detailed documentation of APIs, file formats, and configurations.</li> </ol> <p>\ud83d\udcd6 Learn More</p> <ul> <li> <p>Divio Documentation - Explanations</p> </li> <li> <p>Divio Documentation - How-to Guides</p> </li> <li> <p>Divio Documentation - Tutorials</p> </li> </ul>"},{"location":"#how-to-contribute","title":"How to Contribute","text":"<ol> <li>Read this page and learn how to classify your documentation properly.</li> <li>Create a branch and submit a Pull Request.</li> <li>At least two reviewers' approvals are required before merging.</li> <li>What to change:</li> <li>Create a file in the appropriate category (e.g., <code>docs/explanations/my_article.md</code>).</li> <li>Update the <code>SUMMARY.md</code> file to include your new file path.</li> </ol> <p>We hope this documentation helps you understand, use, and contribute to the project effectively. Together, we are building AI solutions that empower local communities. Happy exploring! \ud83d\ude80</p>"},{"location":"explanations/machine_translation/","title":"SOTA for Machine Translation System","text":""},{"location":"explanations/machine_translation/#1-introduction","title":"1. Introduction","text":"<p>Develop a modular, scalable, and open-source machine translation system for the Moore language, one of the major languages spoken in Burkina Faso. </p> <p>A system with three primary components:</p> <ol> <li> <p>Text-to-Text Translation (MT): Translation between French and Moore.</p> </li> <li> <p>Speech-to-Text (STT): Converting spoken French and Moore into text.</p> </li> <li> <p>Text-to-Speech (TTS): Converting translated text into natural-sounding speech in Moore.</p> </li> </ol> <p>Given that Moore is a low-resource language with limited labeled data --&gt; state-of-the-art (SOTA) deep learning models while leveraging fine-tuning, transfer learning, and domain adaptation techniques.</p> <p>Specificity of Moore language: A lot of monolingual data</p> <p>This document attempts to provide a technical overview for the selection, adaptation, and deployment of models and discusses key challenges, solutions, and future directions.</p>"},{"location":"explanations/machine_translation/#2-text-to-text-translation-mt","title":"2. Text-to-Text Translation (MT)","text":""},{"location":"explanations/machine_translation/#21-model-selection","title":"2.1. Model Selection","text":"<p>SOTA models nowadays are focused on Transformer-based architectures. They have become the standard in machine translation (MT). </p> <p>Interesting options to explore:</p> <ul> <li>mBART-50 (Facebook AI): A multilingual denoising autoencoder trained on 50 languages, capable of unsupervised translation and adaptation.</li> <li>M2M-100 (Facebook AI): A fully multilingual model supporting direct translation between 100 languages without relying on English as an intermediary.</li> <li>MarianMT / OPUS-MT: Open-source Transformer models trained on OPUS parallel corpora, well-suited for low-resource languages.</li> <li> <p>NLLB 200  At scaling machine translation across thousands of language</p> </li> <li> <p>Other interesting option is LLM Lightweight Fine-Tuning \u2013 Mistral (https://mistral.ai/) we can use other models also that are light phi, llama, ....</p> </li> </ul>"},{"location":"explanations/machine_translation/#22-benchmarking-nllb-against-previous-models","title":"2.2. Benchmarking NLLB Against Previous Models","text":""},{"location":"explanations/machine_translation/#evaluation-criteria","title":"Evaluation Criteria:","text":"<ul> <li>Model Size and Inference Speed: </li> <li>NLLB: Although larger than some older models, it has been optimized for scalability and often leverages quantization techniques for faster inference on edge devices.  </li> <li> <p>MarianMT/OPUS-MT: Generally lighter in terms of model parameters, which might be beneficial in resource-constrained environments, albeit sometimes at the cost of translation accuracy.</p> </li> <li> <p>Adaptability to Low-Resource Languages:   NLLB is designed explicitly with low-resource languages in mind, offering tailored fine-tuning and domain adaptation strategies that give it an edge over more general models like mBART-50 and M2M-100 when applied to Moore.</p> </li> <li> <p>Ease of Integration and Fine-Tuning:   All models support transfer learning and can be fine-tuned on domain-specific data. However, NLLB's architecture incorporates recent advances in multilingual training, potentially reducing the amount of fine-tuning required to achieve high accuracy.</p> </li> </ul>"},{"location":"explanations/machine_translation/#benchmarking-summary","title":"Benchmarking Summary:","text":"Model BLEU (Low-Resource) Inference Speed Model Size Adaptability to Moore NLLB High Moderate Large (~billions parameters) Excellent mBART-50 Moderate to High Moderate Medium Not tested M2M-100 Moderate Moderate Medium to Large Not tested MarianMT/OPUS-MT Moderate Fast Small Not tested"},{"location":"explanations/machine_translation/#23-techniques-for-improving-translation-performance","title":"2.3. Techniques for Improving Translation Performance","text":"<ul> <li>Fine-tuning on domain-specific data: Training models with a curated French-Moore parallel corpus.</li> <li>Back-translation (Reference): Generating synthetic Moore-to-French translations to increase training data.</li> <li>Data Augmentation &amp; Denoising: Introducing noise, paraphrasing, and synthetic data generation to improve robustness.</li> <li>Adapter Layers (Reference): Training lightweight adapter modules to specialize in Moore translation without modifying the entire model.</li> <li>Self Labelling: Use STT models to transcribe audios</li> </ul>"},{"location":"explanations/machine_translation/#24-evaluation-metrics","title":"2.4. Evaluation Metrics","text":"<ul> <li>BLEU Score (Reference): Measures translation accuracy by comparing model outputs with human translations.</li> <li>CHRF++ (Reference): A character-based metric useful for morphologically rich languages like Moore.</li> <li>Human Evaluation: Moore speakers assess fluency and adequacy.</li> </ul>"},{"location":"explanations/machine_translation/#3-stt-automatic-speech-recognition-for-moore","title":"3. STT \u2013 Automatic Speech Recognition for Moore","text":""},{"location":"explanations/machine_translation/#31-model-selection","title":"3.1. Model Selection","text":""},{"location":"explanations/machine_translation/#311-whisper-openai-pretrained-model-approach","title":"3.1.1. Whisper (OpenAI) \u2013 Pretrained Model Approach","text":"<ul> <li>Whisper: A multilingual ASR model trained on a large and diverse dataset, supporting Moore transcription and direct speech translation.</li> <li>Fine-tuning: We can enhance Whisper\u2019s accuracy on Moore speech by training on additional labeled Moore audio data.</li> </ul>"},{"location":"explanations/machine_translation/#312-wave2vec-20-facebook-ai","title":"3.1.2. Wave2Vec 2.0 (Facebook AI)","text":"<ul> <li>Wav2Vec 2.0: A powerful self-supervised learning framework for speech representations. It has shown excellent results in various ASR tasks and can be fine-tuned for low-resource languages like Moore.</li> </ul>"},{"location":"explanations/machine_translation/#312-from-scratch-stt-model","title":"3.1.2. From-Scratch STT Model","text":"<p>For developing a custom Moore ASR model, we consider:</p> <ul> <li>Acoustic Modeling:</li> <li>Wav2Vec 2.0: Self-supervised speech representation learning.</li> <li> <p>Conformer: A hybrid convolutional and Transformer-based ASR model.</p> </li> <li> <p>Language Modeling:</p> </li> <li>Train a Moore-specific language model using Transformer-based architectures.</li> <li> <p>Lexicon-based Decoding: Improves rare word recognition.</p> </li> <li> <p>End-to-End Architectures: Unified models that combine acoustic and language modeling.</p> </li> </ul>"},{"location":"explanations/machine_translation/#32-data-collection-augmentation","title":"3.2. Data Collection &amp; Augmentation","text":"<ul> <li>Crowdsourced Moore audio datasets.</li> <li>Synthetic speech data augmentation.</li> <li>Phonetic-based augmentation for pronunciation variation coverage.</li> </ul>"},{"location":"explanations/machine_translation/#33-evaluation-metrics","title":"3.3. Evaluation Metrics","text":"<ul> <li>Word Error Rate (WER) \u2013 Measures transcription accuracy.</li> <li>Phoneme Error Rate (PER) \u2013 Useful for phonetic consistency.</li> <li>Real-time Factor (RTF) \u2013 Measures inference speed.</li> </ul>"},{"location":"explanations/machine_translation/#4-tts-speech-synthesis-for-moore","title":"4. TTS \u2013 Speech Synthesis for Moore","text":""},{"location":"explanations/machine_translation/#41-model-selection","title":"4.1. Model Selection","text":"<p>We consider neural TTS models optimized for low-resource languages:</p> <ul> <li>Tacotron 2 (Google) \u2013 Sequence-to-sequence model for natural speech synthesis.</li> <li>FastSpeech 2 (Microsoft) \u2013 Non-autoregressive model for fast inference.</li> <li>VITS \u2013 End-to-end model with prosody control.</li> </ul>"},{"location":"explanations/machine_translation/#42-techniques-for-improvement","title":"4.2. Techniques for Improvement","text":"<ul> <li>Speaker Adaptation: Fine-tune on Moore voice datasets.</li> <li>Prosody &amp; Expressiveness Modeling: Enhancing pitch and tone variation.</li> <li>Multilingual Pretraining: Using models trained on African languages.</li> <li>Data Augmentation:</li> <li>Speech perturbation (speed, pitch, noise)</li> <li>Phoneme-based synthesis</li> </ul>"},{"location":"explanations/machine_translation/#43-evaluation-metrics","title":"4.3. Evaluation Metrics","text":"<ul> <li>MOS (Mean Opinion Score) \u2013 Human evaluation of naturalness.</li> <li>Mel Cepstral Distortion (MCD) \u2013 Measures synthesized speech quality.</li> <li>CER (Character Error Rate) \u2013 Measures intelligibility.</li> </ul>"},{"location":"explanations/machine_translation/#5-pipeline-integration-deployment","title":"5. Pipeline Integration &amp; Deployment","text":""},{"location":"explanations/machine_translation/#ttt","title":"TTT","text":"<pre><code>graph LR;\n    A[Input Source Text ] --&gt; B[Text Preprocessing &amp; Normalization]\n    B --&gt; C[MT Model Selection]\n    C --&gt; D[Translation Model Options: \n        NLLB / mBART-50 / M2M-100 / MarianMT / Fine-tuned Mistral]\n    D --&gt; E[Post-Translation Processing ]\n    E --&gt; F[Output: Translated Text]</code></pre>"},{"location":"explanations/machine_translation/#tts","title":"TTS","text":"<pre><code>graph LR;\n    A[Audio Input] --&gt; B[Audio Preprocessing -Noise Reduction, Normalization];\n    B --&gt; C[ASR Model -Whisper / Wav2Vec2 / Conformer-];\n    C --&gt; D[Language Model Integration];\n    D --&gt; E[Post-processing];\n    E --&gt; F[Output: Transcribed Text];\n</code></pre>"},{"location":"explanations/machine_translation/#6-challenges","title":"6. Challenges","text":"<ul> <li>Lack of Moore training data \u2192 Data collection &amp; augmentation.</li> <li>Dialectal Variations \u2192 Phonetic modeling techniques.</li> <li>Efficient deployment \u2192 Lightweight models.</li> <li>Multimodal learning (Text, Audio, Visual cues).</li> </ul>"},{"location":"how-to/data_storage/","title":"Accessing and Storing Data in our stack","text":"<p>This guide provides a technical walkthrough for interacting with S3-compatible object storage, using Flio S3 as a practical example.  The principles and code snippets are generally applicable to other S3-compatible services like AWS S3.</p>"},{"location":"how-to/data_storage/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have the following:</p> <ul> <li>An S3-Compatible Storage Account: You'll need an account with an S3-compatible storage provider. This guide uses Flio S3 as an example, but you can adapt it for other providers.</li> <li>Access Credentials: Obtain your <code>AWS_ACCESS_KEY_ID</code>, <code>AWS_SECRET_ACCESS_KEY</code>, and the <code>AWS_ENDPOINT_URL_S3</code> for your storage provider.</li> <li>Python Installed: Ensure you have Python 3 installed on your system.</li> <li>Required Python Libraries: Install the necessary libraries using pip:     <pre><code>pip install boto3 s3fs python-dotenv pandas datasets\n</code></pre><ul> <li><code>boto3</code>: The AWS SDK for Python.</li> <li><code>s3fs</code>: A library that provides a file-like interface to S3.</li> <li><code>python-dotenv</code>: For loading environment variables from a <code>.env</code> file.</li> <li><code>pandas</code>: For working with data in a tabular format (CSV).</li> <li><code>datasets</code>: For interacting with datasets, particularly from Hugging Face.</li> </ul> </li> </ul>"},{"location":"how-to/data_storage/#1-understanding-s3-buckets","title":"1. Understanding S3 Buckets","text":"<p>Here's the corrected version with improved clarity and grammar:  </p> <p>An S3 bucket is like a top-level folder in S3-compatible storage. It helps you store and organize everything\u2014files, images, videos, datasets, and more. However, unlike regular folders, an S3 bucket is designed for scalability, durability, and remote access.  </p> <p>Each object inside a bucket has a unique key (path), just like files inside folders. But in S3, there are no actual \"subfolders\"\u2014everything is stored in a flat structure, and folder-like organization is achieved through naming conventions (e.g., <code>project/images/photo.jpg</code>).  </p> <p>Our bucket can be viewed directly on the site Flio at Fly Storage. Simply scroll down on the main page, select Tigris Object Storage, and access your data.  </p> <p> </p> <p>Now, let\u2019s navigate to a specific path and explore the stored data:  </p> <p> </p> <p>So, now that we will store our data and files here, how can we push files directly to the bucket or read them?</p>"},{"location":"how-to/data_storage/#2-setting-up-access-credentials-for-secure-interaction","title":"2. Setting Up Access Credentials for Secure Interaction","text":"<p>To allow programmatic access to your S3 bucket, you'll need to configure access credentials. These are crucial for authentication and ensuring secure machine-to-machine communication.</p>"},{"location":"how-to/data_storage/#essential-credentials","title":"Essential Credentials","text":"<ul> <li><code>AWS_ACCESS_KEY_ID</code>: Your public identifier.</li> <li><code>AWS_SECRET_ACCESS_KEY</code>: Your private secret key (keep this secure!).</li> <li><code>AWS_ENDPOINT_URL_S3</code>: The specific service endpoint. For Flio S3, this is <code>https://fly.storage.tigris.dev</code>.</li> </ul>"},{"location":"how-to/data_storage/#secure-storage-with-env-files","title":"Secure Storage with <code>.env</code> Files","text":"<p>It's highly recommended to store your credentials in a <code>.env</code> file in your root folder to prevent accidentally exposing them in your code.</p> <pre><code># .env\nAWS_ACCESS_KEY_ID=your_public_key_here\nAWS_SECRET_ACCESS_KEY=your_secret_key_here\nAWS_ENDPOINT_URL_S3=weareburkima.dev\n</code></pre> <p>You can load these credentials into your Python environment using the <code>python-dotenv</code> library:</p> <pre><code>from dotenv import load_dotenv\nimport os\n\nload_dotenv(\".env\")  # Load variables from .env\n</code></pre>"},{"location":"how-to/data_storage/#3-configuring-your-s3-client","title":"3. Configuring Your S3 Client","text":"<p>You can interact with S3-compatible storage using various libraries. Two popular options in Python are <code>boto3</code> (the AWS SDK) and <code>s3fs</code> (which provides a file system-like interface).</p>"},{"location":"how-to/data_storage/#using-boto3-for-direct-api-calls","title":"Using <code>boto3</code> for Direct API Calls","text":"<p><code>boto3</code> offers a comprehensive interface for interacting with S3.</p> <pre><code>import boto3\n\naccess_key = os.getenv(\"AWS_ACCESS_KEY_ID\")\nsecret_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\nendpoint_url = os.getenv(\"AWS_ENDPOINT_URL_S3\")\n\ns3_client = boto3.client(\n    \"s3\",\n    aws_access_key_id=access_key,\n    aws_secret_access_key=secret_key,\n    endpoint_url=endpoint_url,\n)\n</code></pre>"},{"location":"how-to/data_storage/#using-s3fs-for-a-file-system-interface","title":"Using <code>s3fs</code> for a File System Interface","text":"<p><code>s3fs</code> allows you to interact with your S3 bucket as if it were a local file system, which can be convenient for certain operations.</p> <pre><code>import s3fs\n\naccess_key = os.getenv(\"AWS_ACCESS_KEY_ID\")\nsecret_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\nendpoint_url = os.getenv(\"AWS_ENDPOINT_URL_S3\")\n\nfs = s3fs.S3FileSystem(\n    key=access_key,\n    secret=secret_key,\n    endpoint_url=endpoint_url\n)\n</code></pre>"},{"location":"how-to/data_storage/#4-listing-files-within-a-bucket","title":"4. Listing Files Within a Bucket","text":"<p>You'll often need to see what files are stored in your bucket. Here's how to do it with both <code>s3fs</code> and <code>boto3</code>.</p>"},{"location":"how-to/data_storage/#listing-with-s3fs","title":"Listing with <code>s3fs</code>","text":"<pre><code>BUCKET_NAME = \"burkimbia\"\nprefix = \"optional/folder/prefix/\"  # Optional: Filter files within a specific folder\n\ntry:\n    files = fs.ls(f\"{BUCKET_NAME}/{prefix}\")\n    print(f\"Files in s3://{BUCKET_NAME}/{prefix}:\")\n    for file in files:\n        print(file)\nexcept Exception as e:\n    print(f\"Error listing files: {e}\")\n</code></pre>"},{"location":"how-to/data_storage/#listing-with-boto3","title":"Listing with <code>boto3</code>","text":"<pre><code>def list_s3_files(s3_client, bucket_name, prefix=\"\"):\n    files = []\n    paginator = s3_client.get_paginator(\"list_objects_v2\")\n    try:\n        for page in paginator.paginate(Bucket=bucket_name, Prefix=prefix):\n            for obj in page.get(\"Contents\", []):\n                files.append(obj[\"Key\"])\n        return files\n    except Exception as e:\n        print(f\"Error listing files: {e}\")\n        return []\n\nBUCKET_NAME = \"burkimbia\"\nprefix = \"sandbox/raw_data/\"\n\nfile_list = list_s3_files(s3_client, BUCKET_NAME, prefix)\nprint(f\"Files in s3://{BUCKET_NAME}/{prefix}:\")\nfor file in file_list:\n    print(file)\n</code></pre>"},{"location":"how-to/data_storage/#5-uploading-files-to-your-bucket","title":"5. Uploading Files to Your Bucket","text":"<p>To store data, you'll need to upload files to your S3 bucket.</p> <pre><code>def upload_file_to_s3(s3_client, local_path, bucket_name, s3_key):\n    if not os.path.exists(local_path):\n        raise FileNotFoundError(f\"Local file not found: {local_path}\")\n    try:\n        s3_client.upload_file(local_path, bucket_name, s3_key)\n        print(f\"Uploaded '{local_path}' to s3://{bucket_name}/{s3_key}\")\n    except Exception as e:\n        print(f\"Error uploading '{local_path}': {e}\")\n\nBUCKET_NAME = \"burkimbia\"\nlocal_file_path = \"path/to/your/local_file.txt\"\ns3_object_key = \"sandbox/cooked_data/uploaded_file.txt\"\n\nupload_file_to_s3(s3_client, local_file_path, BUCKET_NAME, s3_object_key)\n</code></pre>"},{"location":"how-to/data_storage/#6-downloading-files-from-your-bucket","title":"6. Downloading Files from Your Bucket","text":"<p>Retrieving data from your S3 bucket is done by downloading files.</p> <pre><code>def download_file_from_s3(s3_client, bucket_name, s3_key, local_path):\n    os.makedirs(os.path.dirname(local_path), exist_ok=True)\n    try:\n        s3_client.download_file(bucket_name, s3_key, local_path)\n        print(f\"Downloaded 's3://{bucket_name}/{s3_key}' to '{local_path}'\")\n    except Exception as e:\n        print(f\"Error downloading 's3://{bucket_name}/{s3_key}': {e}\")\n\nBUCKET_NAME = \"burkimbia\"\ns3_object_key = \"path/to/your/file/in/s3.txt\"\nlocal_file_path = \"path/to/save/downloaded_file.txt\"\n\ndownload_file_from_s3(s3_client, BUCKET_NAME, s3_object_key, local_file_path)\n</code></pre>"},{"location":"how-to/data_storage/#7-reading-and-writing-data-directly","title":"7. Reading and Writing Data Directly","text":"<p>Often, you'll want to read data directly from S3 into your applications without downloading it first, or write data directly to S3.</p>"},{"location":"how-to/data_storage/#reading-json-with-s3fs","title":"Reading JSON with <code>s3fs</code>","text":"<pre><code>import json\n\nBUCKET_NAME = \"burkimbia\"\ns3_json_path = f\"{BUCKET_NAME}/sandbox/cooked_data/data.json\"\n\ntry:\n    with fs.open(s3_json_path) as f:\n        data = json.load(f)\n        print(\"JSON Data:\", data)\nexcept Exception as e:\n    print(f\"Error reading JSON from {s3_json_path}: {e}\")\n</code></pre>"},{"location":"how-to/data_storage/#reading-and-writing-csv-data-with-pandas","title":"Reading and Writing CSV Data with Pandas","text":"<p>The <code>pandas</code> library can directly read and write CSV files from and to S3 using <code>s3fs</code> under the hood.</p> <pre><code>import pandas as pd\n\nBUCKET_NAME = \"burkimbia\"\ns3_csv_path = f\"{BUCKET_NAME}/sandbox/cooked_data/data.csv\"\ns3_parquet_path = f\"{BUCKET_NAME}/sandbox/cooked_data/data.parquet\"\n\n# Reading CSV\ntry:\n    df = pd.read_csv(\n        s3_csv_path,\n        storage_options={\n            \"key\": access_key,\n            \"secret\": secret_key,\n            \"client_kwargs\": {\"endpoint_url\": endpoint_url}\n        }\n    )\n    print(\"CSV Data:\")\n    print(df.head())\nexcept Exception as e:\n    print(f\"Error reading CSV from {s3_csv_path}: {e}\")\n\n# Writing Parquet\ntry:\n    df.to_parquet(\n        s3_parquet_path,\n        storage_options={\n            \"key\": access_key,\n            \"secret\": secret_key,\n            \"client_kwargs\": {\"endpoint_url\": endpoint_url}\n        }\n    )\n    print(f\"Data written to {s3_parquet_path}\")\nexcept Exception as e:\n    print(f\"Error writing Parquet to {s3_parquet_path}: {e}\")\n</code></pre>"},{"location":"how-to/data_storage/#integration-with-hugging-face-datasets","title":"Integration with Hugging Face Datasets","text":"<p>The <code>datasets</code> library from Hugging Face can also interact with S3 for storing and loading datasets.</p> <pre><code>from datasets import Dataset\n\nBUCKET_NAME = \"your-bucket-name\"\ns3_dataset_path = f\"s3://{BUCKET_NAME}/golden/text_to_text\"\n\ntry:\n    # Assuming you have a Pandas DataFrame 'df'\n    dataset = Dataset.from_pandas(df)\n    dataset.save_to_disk(\n        s3_dataset_path,\n        storage_options={\n            \"key\": access_key,\n            \"secret\": secret_key,\n            \"client_kwargs\": {\"endpoint_url\": endpoint_url}\n        }\n    )\n    print(f\"Hugging Face dataset saved to {s3_dataset_path}\")\n\n    # Load dataset from S3\n    loaded_dataset = Dataset.load_from_disk(\n        s3_dataset_path,\n        storage_options={\n            \"key\": access_key,\n            \"secret\": secret_key,\n            \"client_kwargs\": {\"endpoint_url\": endpoint_url}\n        }\n    )\n    print(\"Loaded Hugging Face dataset:\", loaded_dataset)\n\nexcept Exception as e:\n    print(f\"Error interacting with Hugging Face dataset on {s3_dataset_path}: {e}\")\n</code></pre>"},{"location":"references/","title":"References","text":"<p>Je me references moi-meme comme example MCP crew :)</p>"},{"location":"tutorials/","title":"Example of tuto wtf i don't know i to name","text":""},{"location":"tutorials/#overview","title":"Overview","text":"<p>Briefly describe what this component or feature does.</p>"},{"location":"tutorials/#prerequisites","title":"Prerequisites","text":"<p>List any prerequisites or dependencies required.</p>"},{"location":"tutorials/#usage","title":"Usage","text":"<p>Show how to use this component with code examples.</p> <pre><code># Example code\nfrom your_project import YourComponent\n\ncomponent = YourComponent(param1=value1)\nresult = component.process(data)\n</code></pre>"},{"location":"tutorials/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>param1</code> str <code>None</code> Description of param1 <code>param2</code> int <code>42</code> Description of param2"},{"location":"tutorials/#return-values","title":"Return Values","text":"<p>Describe what is returned and in what format.</p>"},{"location":"tutorials/#examples","title":"Examples","text":"<p>Provide 1-2 complete examples with expected outputs.</p>"},{"location":"tutorials/#common-issues","title":"Common Issues","text":"<p>List common problems and their solutions.</p>"},{"location":"tutorials/#see-also","title":"See Also","text":"<p>Link to related documentation.</p>"}]}