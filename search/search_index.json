{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#welcome","title":"Welcome","text":"<p>Welcome to the technical documentation of the BurkimBbA project. This open-source project is not only about translation but also about building and contributing to the development of local AI for everyone. We aim to collect, process, and translate texts, audio, and videos in the Moor\u00e9 language to create efficient and accessible AI-driven solutions.</p>"},{"location":"#documentation-structure","title":"Documentation Structure","text":"<p>Our documentation follows the golden tech structure proposed by Divio, which organizes content into four main categories:</p> <p></p> <ol> <li>Tutorials: Step-by-step guides to help you get started with the project.</li> <li>How-to guides: Practical instructions to complete specific tasks.</li> <li>Explanations: Insights into key concepts and technical decisions.</li> <li>References: Detailed documentation of APIs, file formats, and configurations.</li> </ol> <p>\ud83d\udcd6 Learn More</p> <ul> <li> <p>Divio Documentation - Explanations</p> </li> <li> <p>Divio Documentation - How-to Guides</p> </li> <li> <p>Divio Documentation - Tutorials</p> </li> </ul>"},{"location":"#how-to-contribute","title":"How to Contribute","text":"<ol> <li>Read this page and learn how to classify your documentation properly.</li> <li>Create a branch and submit a Pull Request.</li> <li>At least two reviewers' approvals are required before merging.</li> <li>What to change:</li> <li>Create a file in the appropriate category (e.g., <code>docs/explanations/my_article.md</code>).</li> <li>Update the <code>SUMMARY.md</code> file to include your new file path.</li> </ol> <p>We hope this documentation helps you understand, use, and contribute to the project effectively. Together, we are building AI solutions that empower local communities. Happy exploring! \ud83d\ude80</p>"},{"location":"explanations/deploiement_llm_serverless/","title":"deploiement llm sans cluster","text":"<p>Notre equipe Burkimbia doit servir des modeles pour des langues africaines sans exploser un budget limite a vingt euros par mois. Ce billet raconte comment nous avons arbitre entre plusieurs fournisseurs et pourquoi le mode serverless reste aujourd'hui notre garde fou. Pas de pas a pas ici, uniquement les choix strategiques et leurs impacts.</p>"},{"location":"explanations/deploiement_llm_serverless/#nos-contraintes-terrain","title":"nos contraintes terrain","text":"<ul> <li>usage intermittent: environ trois mille requetes mensuelles, chacune autour de 8.5 secondes de calcul GPU.</li> <li>public fragile: trente pour cent des requetes sont offertes pour des apprenantes et apprenants sans moyens.</li> <li>equipe reduite: pas de temps pour maintenir un cluster ECS ou EKS.</li> <li>transparence: tout cout doit etre justifiable pour nos financeurs.</li> </ul> <p>Ces contraintes rendent toute facturation continue impraticable. Louer un T4 en permanence chez AWS ou Azure revient a plus de 360 dollars mensuels. Avec le mode serverless, seules les secondes effectives sont facturees.</p>"},{"location":"explanations/deploiement_llm_serverless/#comparatif-rapide-des-plateformes","title":"comparatif rapide des plateformes","text":"plateforme gpu modele de facturation cout mensuel estime cout pour 1000 requetes aws ec2 g4dn.xlarge 720 heures payees 378.72 usd 126.24 usd azure vm nc4as_t4_v3 720 heures payees 378.72 usd 126.24 usd hugging face endpoints t4 720 heures payees 360.00 usd 120.00 usd runpod flex rtx a6000 a la seconde 8.67 usd 2.89 usd <p>Les chiffres proviennent des grilles officielles d'octobre 2025. Seul RunPod Flex respecte notre budget tout en offrant une A6000 pour les modeles de 7b quantifies.</p>"},{"location":"explanations/deploiement_llm_serverless/#lessons-retenues-apres-un-incident-couteux","title":"lessons retenues apres un incident couteux","text":"<p>Un oubli de execution timeout a prolonge un worker RunPod pendant plusieurs jours. Plus de cinq mille requetes de ping ont maintenu la session ouverte et genere des frais serverless importants. Depuis, nous appliquons trois regles:</p> <ol> <li>fixer idle_timeout et execution_timeout pour chaque endpoint.</li> <li>limiter max_workers a 1 tant que la charge reste faible.</li> <li>monitorer la frequence de ping via nos journaux et couper tout traffic suspect.</li> </ol>"},{"location":"explanations/deploiement_llm_serverless/#images-docker-minimalistes","title":"images docker minimalistes","text":"<p>Nos premieres images PyTorch depassaient neuf gigaoctets. Cela rallongeait chaque cold start. Nous sommes passes a une base Nvidia CUDA puis installons seulement les builds necessaires (Torch, BitsAndBytes, SentencePiece). Chaque image contient directement le modele quantifie pour eviter un telechargement au demarrage. Avec cette approche, les cold start descendent sous quinze secondes sur A6000 et les surprises de facturation disparaissent.</p>"},{"location":"explanations/deploiement_llm_serverless/#quantification-et-matrice-gpu","title":"quantification et matrice gpu","text":"<p>Nous ne lancons aucun modele non quantifie sur RunPod. Mistral 7b passe de vingt gigaoctets en FP16 a environ quatre gigaoctets en INT4. Cela rend l'image plus legere et nous autorise a rester sur A6000 ou meme A4000 pour des charges plus modestes. La regle interne: quantifier d'abord, tester ensuite le plus petit GPU disponible, ne monter en gamme que si la latence explose.</p>"},{"location":"explanations/deploiement_llm_serverless/#suivi-budgetaire-en-temps-reel","title":"suivi budgetaire en temps reel","text":"<p>Trois indicateurs sont exposes dans notre tableau de bord Metabase:</p> <ul> <li>cold_start_seconds: objectif sous quinze secondes.</li> <li>gpu_seconds_total: converti en cout temps reel en euros.</li> <li>cost_per_1000_requests: alerte quand on depasse 3 usd.</li> </ul> <p>Nous declenchons une alerte Slack quand quatre-vingts pour cent du budget mensuel sont consommes. La transparence vis-a-vis des partenaires rend plus faciles les demandes de financement ponctuel.</p>"},{"location":"explanations/deploiement_llm_serverless/#stack-hybride-recommandee","title":"stack hybride recommandee","text":"<ul> <li>production: endpoints RunPod Serverless en RTX A6000 avec timeouts serre.</li> <li>prototypes publics: Hugging Face Spaces ZeroGPU a dix dollars pour heberger des demos Gradio.</li> <li>experiments internes: instances locales consumer GPU pour iterer sur les quantifications.</li> </ul> <p>Cette combinaison maintient les couts sous vingt euros par mois tout en gardant une vitrine publique simple a partager.</p>"},{"location":"explanations/deploiement_llm_serverless/#checklist-strategique-avant-tout-nouveau-modele","title":"checklist strategique avant tout nouveau modele","text":"<ol> <li>verifier la taille en FP16 puis en INT4 et mettre a jour le plan de build Docker.</li> <li>recalculer le cout par mille requetes avec les nouvelles latences mesurees.</li> <li>mettre a jour idle_timeout, execution_timeout et les limites max_workers.</li> <li>ajouter le modele au tableau de bord couts et tester les alertes.</li> <li>documenter l'impact pour les financeurs avant le lancement public.</li> </ol> <p>Rester frugal n'est pas un choix ideologique mais une condition de survie pour une organisation a but non lucratif. Cette fiche conceptuelle doit guider toute nouvelle initiative autour des LLM chez Burkimbia.</p>"},{"location":"explanations/machine_translation/","title":"machine translation stack","text":"<p>Cette note expose notre approche pour un systeme de traduction couvrant le francais et le moore. L'objectif est de rester opensource, modulaire et realiste pour une equipe reduite.</p>"},{"location":"explanations/machine_translation/#objectifs-du-systeme","title":"objectifs du systeme","text":"<ul> <li>text-to-text: traduction bidirectionnelle entre francais et moore.</li> <li>speech-to-text: transcription des audios francais et moore.</li> <li>text-to-speech: generation de parole naturelle en moore.</li> </ul> <p>Les ressources etiquetees restent rares, d'ou l'obligation de miser sur le transfert d'apprentissage et la quantification.</p>"},{"location":"explanations/machine_translation/#text-to-text","title":"text to text","text":""},{"location":"explanations/machine_translation/#modeles-a-privilegier","title":"modeles a privilegier","text":"<ul> <li><code>nllb-200</code> pour la couverture des langues peu dotees.</li> <li><code>mbart-50</code> comme base polyvalente facile a affiner.</li> <li><code>m2m-100</code> pour eviter un pivot par l'anglais.</li> <li><code>marianmt</code> lorsque l'on cherche un modele compacte.</li> <li><code>mistral 7b</code> ou autres <code>llm</code> legers apres quantification <code>int4</code>.</li> </ul>"},{"location":"explanations/machine_translation/#ameliorations-cibles","title":"ameliorations cibles","text":"<ul> <li>constitution d'un corpus parallele francais-moore et fine tuning specifique.</li> <li>back translation pour augmenter artificiellement la portion moore.</li> <li>couches <code>adapter</code> pour personnaliser sans toucher a tous les poids.</li> <li>auto etiquetage via notre pipeline <code>stt</code> pour produire du parallele low-cost.</li> </ul>"},{"location":"explanations/machine_translation/#mesures-de-qualite","title":"mesures de qualite","text":"<ul> <li><code>bleu</code> et <code>chrf++</code> pour suivre les progres.</li> <li>evaluation humaine par nos locuteurs natifs.</li> <li>suivi de la latence pour garantir une inference acceptable sur <code>a4000</code> ou <code>a6000</code>.</li> </ul>"},{"location":"explanations/machine_translation/#speech-to-text","title":"speech to text","text":""},{"location":"explanations/machine_translation/#options-de-modele","title":"options de modele","text":"<ul> <li><code>whisper</code> adapte au moore apres un leger affinement.</li> <li><code>wav2vec 2.0</code> pour profiter de l'apprentissage auto supervise.</li> <li>architectures <code>conformer</code> lorsque l'on veut pousser la precision.</li> </ul>"},{"location":"explanations/machine_translation/#collecte-et-augmentation","title":"collecte et augmentation","text":"<ul> <li>campagnes de crowdsourcing dans les communautes partenaires.</li> <li>generation synthetique avec modification de vitesse, hauteur ou bruit.</li> <li>augmentation phonemique pour couvrir les variantes dialectales.</li> </ul>"},{"location":"explanations/machine_translation/#suivi-de-performance","title":"suivi de performance","text":"<ul> <li><code>wer</code> comme indicateur principal.</li> <li><code>per</code> pour surveiller les confusions phonemiques.</li> <li><code>rtf</code> pour garantir une inference proche du temps reel.</li> </ul>"},{"location":"explanations/machine_translation/#text-to-speech","title":"text to speech","text":""},{"location":"explanations/machine_translation/#modeles-candidats","title":"modeles candidats","text":"<ul> <li><code>tacotron 2</code> pour la qualite vocale.</li> <li><code>fastspeech 2</code> pour la rapidite.</li> <li><code>vits</code> quand on veut un pipeline de bout en bout.</li> </ul>"},{"location":"explanations/machine_translation/#leviers-de-progression","title":"leviers de progression","text":"<ul> <li>adaptation par locuteur a partir de corpus moore collectes localement.</li> <li>modelisation de la prosodie pour un rendu expressif.</li> <li>augmentation avec perturbations audio et representations phonemiques.</li> </ul>"},{"location":"explanations/machine_translation/#metriques-clefs","title":"metriques clefs","text":"<ul> <li><code>mos</code> obtenu via ecoutes internes.</li> <li><code>mcd</code> pour suivre la qualite spectrale.</li> <li><code>cer</code> afin d'assurer l'intelligibilite.</li> </ul>"},{"location":"explanations/machine_translation/#integration-pipeline","title":"integration pipeline","text":""},{"location":"explanations/machine_translation/#traduction-ecrite","title":"traduction ecrite","text":"<pre><code>graph LR;\n    A[texte source] --&gt; B[normalisation]\n    B --&gt; C[selection du modele]\n    C --&gt; D[post-traitement]\n    D --&gt; E[texte traduit]</code></pre>"},{"location":"explanations/machine_translation/#chaine-voix","title":"chaine voix","text":"<pre><code>graph LR;\n    A[audio] --&gt; B[pretraitement]\n    B --&gt; C[modele stt]\n    C --&gt; D[modele de langage]\n    D --&gt; E[post-traitement]\n    E --&gt; F[transcription]</code></pre>"},{"location":"explanations/machine_translation/#defis-a-surveiller","title":"defis a surveiller","text":"<ul> <li>penurie de donnees moore etiquetees.</li> <li>variabilite dialectale entre regions.</li> <li>contraintes de deploiement sur du materiel limite.</li> <li>besoin futur de support multimodal (texte, audio, visuel).</li> </ul>"},{"location":"how-to/data_storage/","title":"Accessing and Storing Data in our stack","text":"<p>This guide provides a technical walkthrough for interacting with S3-compatible object storage, using Flio S3 as a practical example.  The principles and code snippets are generally applicable to other S3-compatible services like AWS S3.</p>"},{"location":"how-to/data_storage/#prerequisites","title":"Prerequisites","text":"<p>Before you begin, ensure you have the following:</p> <ul> <li>An S3-Compatible Storage Account: You'll need an account with an S3-compatible storage provider. This guide uses Flio S3 as an example, but you can adapt it for other providers.</li> <li>Access Credentials: Obtain your <code>AWS_ACCESS_KEY_ID</code>, <code>AWS_SECRET_ACCESS_KEY</code>, and the <code>AWS_ENDPOINT_URL_S3</code> for your storage provider.</li> <li>Python Installed: Ensure you have Python 3 installed on your system.</li> <li>Required Python Libraries: Install the necessary libraries using pip:     <pre><code>pip install boto3 s3fs python-dotenv pandas datasets\n</code></pre><ul> <li><code>boto3</code>: The AWS SDK for Python.</li> <li><code>s3fs</code>: A library that provides a file-like interface to S3.</li> <li><code>python-dotenv</code>: For loading environment variables from a <code>.env</code> file.</li> <li><code>pandas</code>: For working with data in a tabular format (CSV).</li> <li><code>datasets</code>: For interacting with datasets, particularly from Hugging Face.</li> </ul> </li> </ul>"},{"location":"how-to/data_storage/#1-understanding-s3-buckets","title":"1. Understanding S3 Buckets","text":"<p>Here's the corrected version with improved clarity and grammar:  </p> <p>An S3 bucket is like a top-level folder in S3-compatible storage. It helps you store and organize everything\u2014files, images, videos, datasets, and more. However, unlike regular folders, an S3 bucket is designed for scalability, durability, and remote access.  </p> <p>Each object inside a bucket has a unique key (path), just like files inside folders. But in S3, there are no actual \"subfolders\"\u2014everything is stored in a flat structure, and folder-like organization is achieved through naming conventions (e.g., <code>project/images/photo.jpg</code>).  </p> <p>Our bucket can be viewed directly on the site Flio at Fly Storage. Simply scroll down on the main page, select Tigris Object Storage, and access your data.  </p> <p> </p> <p>Now, let\u2019s navigate to a specific path and explore the stored data:  </p> <p> </p> <p>So, now that we will store our data and files here, how can we push files directly to the bucket or read them?</p>"},{"location":"how-to/data_storage/#2-setting-up-access-credentials-for-secure-interaction","title":"2. Setting Up Access Credentials for Secure Interaction","text":"<p>To allow programmatic access to your S3 bucket, you'll need to configure access credentials. These are crucial for authentication and ensuring secure machine-to-machine communication.</p>"},{"location":"how-to/data_storage/#essential-credentials","title":"Essential Credentials","text":"<ul> <li><code>AWS_ACCESS_KEY_ID</code>: Your public identifier.</li> <li><code>AWS_SECRET_ACCESS_KEY</code>: Your private secret key (keep this secure!).</li> <li><code>AWS_ENDPOINT_URL_S3</code>: The specific service endpoint. For Flio S3, this is <code>https://fly.storage.tigris.dev</code>.</li> </ul>"},{"location":"how-to/data_storage/#secure-storage-with-env-files","title":"Secure Storage with <code>.env</code> Files","text":"<p>It's highly recommended to store your credentials in a <code>.env</code> file in your root folder to prevent accidentally exposing them in your code.</p> <pre><code># .env\nAWS_ACCESS_KEY_ID=your_public_key_here\nAWS_SECRET_ACCESS_KEY=your_secret_key_here\nAWS_ENDPOINT_URL_S3=weareburkima.dev\n</code></pre> <p>You can load these credentials into your Python environment using the <code>python-dotenv</code> library:</p> <pre><code>from dotenv import load_dotenv\nimport os\n\nload_dotenv(\".env\")  # Load variables from .env\n</code></pre>"},{"location":"how-to/data_storage/#3-configuring-your-s3-client","title":"3. Configuring Your S3 Client","text":"<p>You can interact with S3-compatible storage using various libraries. Two popular options in Python are <code>boto3</code> (the AWS SDK) and <code>s3fs</code> (which provides a file system-like interface).</p>"},{"location":"how-to/data_storage/#using-boto3-for-direct-api-calls","title":"Using <code>boto3</code> for Direct API Calls","text":"<p><code>boto3</code> offers a comprehensive interface for interacting with S3.</p> <pre><code>import boto3\n\naccess_key = os.getenv(\"AWS_ACCESS_KEY_ID\")\nsecret_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\nendpoint_url = os.getenv(\"AWS_ENDPOINT_URL_S3\")\n\ns3_client = boto3.client(\n    \"s3\",\n    aws_access_key_id=access_key,\n    aws_secret_access_key=secret_key,\n    endpoint_url=endpoint_url,\n)\n</code></pre>"},{"location":"how-to/data_storage/#using-s3fs-for-a-file-system-interface","title":"Using <code>s3fs</code> for a File System Interface","text":"<p><code>s3fs</code> allows you to interact with your S3 bucket as if it were a local file system, which can be convenient for certain operations.</p> <pre><code>import s3fs\n\naccess_key = os.getenv(\"AWS_ACCESS_KEY_ID\")\nsecret_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\nendpoint_url = os.getenv(\"AWS_ENDPOINT_URL_S3\")\n\nfs = s3fs.S3FileSystem(\n    key=access_key,\n    secret=secret_key,\n    endpoint_url=endpoint_url\n)\n</code></pre>"},{"location":"how-to/data_storage/#4-listing-files-within-a-bucket","title":"4. Listing Files Within a Bucket","text":"<p>You'll often need to see what files are stored in your bucket. Here's how to do it with both <code>s3fs</code> and <code>boto3</code>.</p>"},{"location":"how-to/data_storage/#listing-with-s3fs","title":"Listing with <code>s3fs</code>","text":"<pre><code>BUCKET_NAME = \"burkimbia\"\nprefix = \"optional/folder/prefix/\"  # Optional: Filter files within a specific folder\n\ntry:\n    files = fs.ls(f\"{BUCKET_NAME}/{prefix}\")\n    print(f\"Files in s3://{BUCKET_NAME}/{prefix}:\")\n    for file in files:\n        print(file)\nexcept Exception as e:\n    print(f\"Error listing files: {e}\")\n</code></pre>"},{"location":"how-to/data_storage/#listing-with-boto3","title":"Listing with <code>boto3</code>","text":"<pre><code>def list_s3_files(s3_client, bucket_name, prefix=\"\"):\n    files = []\n    paginator = s3_client.get_paginator(\"list_objects_v2\")\n    try:\n        for page in paginator.paginate(Bucket=bucket_name, Prefix=prefix):\n            for obj in page.get(\"Contents\", []):\n                files.append(obj[\"Key\"])\n        return files\n    except Exception as e:\n        print(f\"Error listing files: {e}\")\n        return []\n\nBUCKET_NAME = \"burkimbia\"\nprefix = \"sandbox/raw_data/\"\n\nfile_list = list_s3_files(s3_client, BUCKET_NAME, prefix)\nprint(f\"Files in s3://{BUCKET_NAME}/{prefix}:\")\nfor file in file_list:\n    print(file)\n</code></pre>"},{"location":"how-to/data_storage/#5-uploading-files-to-your-bucket","title":"5. Uploading Files to Your Bucket","text":"<p>To store data, you'll need to upload files to your S3 bucket.</p> <pre><code>def upload_file_to_s3(s3_client, local_path, bucket_name, s3_key):\n    if not os.path.exists(local_path):\n        raise FileNotFoundError(f\"Local file not found: {local_path}\")\n    try:\n        s3_client.upload_file(local_path, bucket_name, s3_key)\n        print(f\"Uploaded '{local_path}' to s3://{bucket_name}/{s3_key}\")\n    except Exception as e:\n        print(f\"Error uploading '{local_path}': {e}\")\n\nBUCKET_NAME = \"burkimbia\"\nlocal_file_path = \"path/to/your/local_file.txt\"\ns3_object_key = \"sandbox/cooked_data/uploaded_file.txt\"\n\nupload_file_to_s3(s3_client, local_file_path, BUCKET_NAME, s3_object_key)\n</code></pre>"},{"location":"how-to/data_storage/#6-downloading-files-from-your-bucket","title":"6. Downloading Files from Your Bucket","text":"<p>Retrieving data from your S3 bucket is done by downloading files.</p> <pre><code>def download_file_from_s3(s3_client, bucket_name, s3_key, local_path):\n    os.makedirs(os.path.dirname(local_path), exist_ok=True)\n    try:\n        s3_client.download_file(bucket_name, s3_key, local_path)\n        print(f\"Downloaded 's3://{bucket_name}/{s3_key}' to '{local_path}'\")\n    except Exception as e:\n        print(f\"Error downloading 's3://{bucket_name}/{s3_key}': {e}\")\n\nBUCKET_NAME = \"burkimbia\"\ns3_object_key = \"path/to/your/file/in/s3.txt\"\nlocal_file_path = \"path/to/save/downloaded_file.txt\"\n\ndownload_file_from_s3(s3_client, BUCKET_NAME, s3_object_key, local_file_path)\n</code></pre>"},{"location":"how-to/data_storage/#7-reading-and-writing-data-directly","title":"7. Reading and Writing Data Directly","text":"<p>Often, you'll want to read data directly from S3 into your applications without downloading it first, or write data directly to S3.</p>"},{"location":"how-to/data_storage/#reading-json-with-s3fs","title":"Reading JSON with <code>s3fs</code>","text":"<pre><code>import json\n\nBUCKET_NAME = \"burkimbia\"\ns3_json_path = f\"{BUCKET_NAME}/sandbox/cooked_data/data.json\"\n\ntry:\n    with fs.open(s3_json_path) as f:\n        data = json.load(f)\n        print(\"JSON Data:\", data)\nexcept Exception as e:\n    print(f\"Error reading JSON from {s3_json_path}: {e}\")\n</code></pre>"},{"location":"how-to/data_storage/#reading-and-writing-csv-data-with-pandas","title":"Reading and Writing CSV Data with Pandas","text":"<p>The <code>pandas</code> library can directly read and write CSV files from and to S3 using <code>s3fs</code> under the hood.</p> <pre><code>import pandas as pd\n\nBUCKET_NAME = \"burkimbia\"\ns3_csv_path = f\"{BUCKET_NAME}/sandbox/cooked_data/data.csv\"\ns3_parquet_path = f\"{BUCKET_NAME}/sandbox/cooked_data/data.parquet\"\n\n# Reading CSV\ntry:\n    df = pd.read_csv(\n        s3_csv_path,\n        storage_options={\n            \"key\": access_key,\n            \"secret\": secret_key,\n            \"client_kwargs\": {\"endpoint_url\": endpoint_url}\n        }\n    )\n    print(\"CSV Data:\")\n    print(df.head())\nexcept Exception as e:\n    print(f\"Error reading CSV from {s3_csv_path}: {e}\")\n\n# Writing Parquet\ntry:\n    df.to_parquet(\n        s3_parquet_path,\n        storage_options={\n            \"key\": access_key,\n            \"secret\": secret_key,\n            \"client_kwargs\": {\"endpoint_url\": endpoint_url}\n        }\n    )\n    print(f\"Data written to {s3_parquet_path}\")\nexcept Exception as e:\n    print(f\"Error writing Parquet to {s3_parquet_path}: {e}\")\n</code></pre>"},{"location":"how-to/data_storage/#integration-with-hugging-face-datasets","title":"Integration with Hugging Face Datasets","text":"<p>The <code>datasets</code> library from Hugging Face can also interact with S3 for storing and loading datasets.</p> <pre><code>from datasets import Dataset\n\nBUCKET_NAME = \"your-bucket-name\"\ns3_dataset_path = f\"s3://{BUCKET_NAME}/golden/text_to_text\"\n\ntry:\n    # Assuming you have a Pandas DataFrame 'df'\n    dataset = Dataset.from_pandas(df)\n    dataset.save_to_disk(\n        s3_dataset_path,\n        storage_options={\n            \"key\": access_key,\n            \"secret\": secret_key,\n            \"client_kwargs\": {\"endpoint_url\": endpoint_url}\n        }\n    )\n    print(f\"Hugging Face dataset saved to {s3_dataset_path}\")\n\n    # Load dataset from S3\n    loaded_dataset = Dataset.load_from_disk(\n        s3_dataset_path,\n        storage_options={\n            \"key\": access_key,\n            \"secret\": secret_key,\n            \"client_kwargs\": {\"endpoint_url\": endpoint_url}\n        }\n    )\n    print(\"Loaded Hugging Face dataset:\", loaded_dataset)\n\nexcept Exception as e:\n    print(f\"Error interacting with Hugging Face dataset on {s3_dataset_path}: {e}\")\n</code></pre>"},{"location":"how-to/inference/","title":"Documentation API BurkimbIA - Services d'Inf\u00e9rence","text":""},{"location":"how-to/inference/#repositories-importants","title":"Repositories Importants","text":"<p>Pour modifier nos API, vous devez conna\u00eetre ces deux repositories :</p> <p>-- inference-endpoints (prioritaire) : Contient les mod\u00e8les et le code de d\u00e9ploiement sur RunPod -- bia-backend : API Gateway d\u00e9ploy\u00e9 sur Fly.io</p> <p>\u26a0\ufe0f Important : Toute modification dans <code>inference-endpoints</code> doit \u00eatre r\u00e9percut\u00e9e dans <code>bia-backend</code> si n\u00e9cessaire.</p>"},{"location":"how-to/inference/#architecture","title":"Architecture","text":"<p>architecture</p> <p>L'infrastructure repose sur deux composants principaux :</p> <ul> <li>inference-endpoints : repository contenant les mod\u00e8les et le code pour les servir sur RunPod (serverless).</li> <li>bia-backend : application FastAPI d\u00e9ploy\u00e9e sur Fly.io, responsable de l'authentification, des logs et du rate limiting.</li> </ul> <p>Nous recommandons d'utiliser <code>bia-backend</code> comme API gateway pour centraliser l'authentification et le contr\u00f4le d'acc\u00e8s.</p> <pre><code>flowchart TD\n    %% Define styles for different components\n    %% Bleu pour le Client, Violet pour bia-backend, Vert pour inference-endpoints\n    classDef userStyle fill:#e1f5fe,stroke:#0277bd,stroke-width:3px,color:#000\n    classDef backendStyle fill:#f3e5f5,stroke:#7b1fa2,stroke-width:2px,color:#000\n    classDef inferenceStyle fill:#e8f5e8,stroke:#388e3c,stroke-width:3px,color:#000\n\n    %% Components\n    U[\ud83d\udc64 Client&lt;br/&gt;Applications tierces]\n    A[\ud83d\udd10 Auth Service&lt;br/&gt;bia-backend&lt;br/&gt;JWT / API Key]\n    B[\u2699\ufe0f API Gateway&lt;br/&gt;bia-backend&lt;br/&gt;Rate Limiting &amp; Logging]\n    T[\ud83c\udf10 Translation&lt;br/&gt;inference-endpoints&lt;br/&gt;RunPod]\n    S[\ud83c\udfa4 Transcription&lt;br/&gt;inference-endpoints&lt;br/&gt;RunPod]\n    V[\ud83d\udd0a TTS&lt;br/&gt;inference-endpoints&lt;br/&gt;RunPod]\n\n    %% Relationships - Flux num\u00e9rot\u00e9 de 1 \u00e0 6\n    U -.-&gt;|\"1\ufe0f\u20e3 Authenticate\"| A\n    U --&gt;|\"2\ufe0f\u20e3 Request\"| B\n    B -.-&gt;|\"3\ufe0f\u20e3 Verify\"| A\n    B ==&gt;|\"4\ufe0f\u20e3 Forward\"| T\n    B ==&gt;|\"4\ufe0f\u20e3 Forward\"| S\n    B ==&gt;|\"4\ufe0f\u20e3 Forward\"| V\n    T ==&gt;|\"5\ufe0f\u20e3 Response\"| B\n    S ==&gt;|\"5\ufe0f\u20e3 Response\"| B\n    V ==&gt;|\"5\ufe0f\u20e3 Response\"| B\n    B --&gt;|\"6\ufe0f\u20e3 Result\"| U\n\n    %% Apply styles\n    class U userStyle\n    class A,B backendStyle\n    class T,S,V inferenceStyle</code></pre>"},{"location":"how-to/inference/#services-et-modeles","title":"Services et Mod\u00e8les","text":"<p>La table ci-dessous pr\u00e9sente les mod\u00e8les ML utilis\u00e9s par service :</p> Service Mod\u00e8le Taille Fonction Translation <code>burkimbia/BIA-NLLB-600M-5E</code> 600M Traduction Fran\u00e7ais\u2194Moor\u00e9 Translation <code>burkimbia/BIA-MISTRAL-7B-SACHI</code> 7B Traduction instruction-tuned Transcription <code>burkimbia/BIA-WHISPER-LARGE-SACHI_V2</code> 1.5B Audio\u2192Texte TTS <code>burkimbia/BIA-SPARK-TTS-V2</code> 0.5B Texte\u2192Audio"},{"location":"how-to/inference/#deploiement-sur-runpod","title":"D\u00e9ploiement sur RunPod","text":""},{"location":"how-to/inference/#procedure","title":"Proc\u00e9dure","text":"<p>\u00c9tapes du d\u00e9ploiement manuel via l'interface RunPod</p> <ol> <li>D\u00e9ployer l'image Docker via l'interface RunPod</li> </ol> <p></p> <ol> <li>S\u00e9lectionner l'option de service appropri\u00e9e</li> </ol> <p></p> <ol> <li>Lancer la CI et attendre le succ\u00e8s du build</li> </ol> <p></p> <ol> <li>V\u00e9rifier le statut du worker (passe de initializing \u00e0 idle / running)</li> </ol> <p></p> <ol> <li>R\u00e9cup\u00e9rer le service ID depuis l'interface RunPod ou les logs CI</li> </ol> <p></p> <p></p>"},{"location":"how-to/inference/#configuration","title":"Configuration","text":"<p>Le fichier runpod-config.json d\u00e9finit les endpoints, images Docker et ressources GPU.</p> <p>\u26a0\ufe0f Apr\u00e8s chaque d\u00e9ploiement : Le service ID change. Mettre \u00e0 jour les variables d'environnement dans bia-backend. </p>"},{"location":"how-to/inference/#appel-direct-possible-mais-non-utilise","title":"Appel direct possible (mais non utilis\u00e9)","text":"<p>Il est techniquement possible d'appeler directement les endpoints RunPod (r\u00e9serv\u00e9 aux membres BurkimbIA)</p> <pre><code># Exemple d'appel direct RunPod (pour r\u00e9f\u00e9rence uniquement)\ncurl -X POST https://api.runpod.ai/v2/{endpoint_id}/run \\\n    -H 'Content-Type: application/json' \\\n    -H 'Authorization: Bearer TOKEN_RUNPOD' \\\n    -d '{\n      \"input\": {\n        \"text\": \"Bonjour le monde\",\n        \"model_id\": \"burkimbia/BIA-NLLB-1.3B-7E\",\n        \"src_lang\": \"fr_Latn\",\n        \"tgt_lang\": \"moor_Latn\"\n      }\n    }'\n</code></pre> <p>Probl\u00e8me : RunPod fournit une seule API key pour tous les services, sans branding ni contr\u00f4le d'acc\u00e8s personnalis\u00e9.</p> <p>Solution : C'est pourquoi nous utilisons bia-backend comme API Gateway, qui permet : - Gestion multi-utilisateurs avec authentification - Branding personnalis\u00e9 (<code>api.burkimbia.com</code>) - Rate limiting par utilisateur - Logs et monitoring centralis\u00e9s</p>"},{"location":"how-to/inference/#utilisation-via-bia-backend","title":"Utilisation via bia-backend","text":"<p>L'API Gateway est le point d'entr\u00e9e recommand\u00e9 pour tous les services</p> <pre><code>curl -X POST https://api.burkimbia.com/v1/translate \\\n  -H \"Authorization: Bearer BIA_VOTRE_TOKEN\" \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\n    \"text\": \"Bonjour\",\n    \"src_lang\": \"fr\",\n    \"tgt_lang\": \"moore\"\n  }'\n</code></pre> <p>Les tokens JWT ou cl\u00e9s API sont obtenus via le backend</p>"},{"location":"how-to/inference/#debugging","title":"Debugging","text":""},{"location":"how-to/inference/#backend-bia-backend","title":"Backend (bia-backend)","text":"<p>Utiliser Grafana pour les logs d\u00e9taill\u00e9s</p> <p>Utiliser les logs Grafana sur Fly.io : - URL : https://fly.io/apps/bia-backend/monitoring - \u23f1\ufe0f Le chargement prend du temps mais fournit des informations d\u00e9taill\u00e9es</p>"},{"location":"how-to/inference/#services-dinference-runpod","title":"Services d'inf\u00e9rence (RunPod)","text":"<p>Aller dans Manage Serverless puis s\u00e9lectionner le service.</p> <p>Playground : Tester les requ\u00eates interactivement</p> <p></p> <p>Logs : Consulter les logs d'ex\u00e9cution</p> <p></p> <p>Workers Status : V\u00e9rifier l'\u00e9tat des workers</p>"},{"location":"how-to/inference/#erreurs-courantes-github-actions-cicd","title":"Erreurs Courantes (GitHub Actions CI/CD)","text":""},{"location":"how-to/inference/#1-limite-de-workers-depassee","title":"1. Limite de workers d\u00e9pass\u00e9e","text":"<pre><code>Checking for existing endpoint...\nCreating endpoint...\n\u274c Error creating endpoint: 500 Server Error: Internal Server Error for url: https://rest.runpod.io/v1/endpoints\nResponse: {\"error\":\"create endpoint: create endpoint: graphql: Max workers across all endpoints must not exceed your workers quota (5). Reduce the max workers for other endpoints or lower the max worker count for this endpoint to at most 0.\",\"status\":500}\n</code></pre> <p>Cause : Pour les comptes tiers comme le n\u00f4tre, nous sommes limit\u00e9s \u00e0 5 workers maximum.</p> <p>Solution : Supprimer quelques workers existants sur RunPod avant de red\u00e9ployer.</p>"},{"location":"how-to/inference/#2-template-introuvable-service-non-pret","title":"2. Template introuvable (service non pr\u00eat)","text":"<pre><code>Checking for existing endpoint...\nCreating endpoint...\n\u274c Error creating endpoint: 500 Server Error: Internal Server Error for url: https://rest.runpod.io/v1/endpoints\nResponse: {\"error\":\"create endpoint: create endpoint: graphql: Unable to find template\",\"status\":500}\n</code></pre> <p>Cause : Le service n'est pas encore pr\u00eat sur RunPod.</p> <p>Solution : Relancer simplement la CI de red\u00e9ploiement.</p>"},{"location":"how-to/inference/#authentification","title":"Authentification","text":"<p>Le backend supporte plusieurs modes d'authentification</p> <ul> <li>Email/mot de passe</li> <li>OAuth Google</li> <li>Cl\u00e9s API pour applications tierces</li> </ul>"},{"location":"references/","title":"documentation style","text":"<p>Cette section sert d'exemple minimal pour tester la navigation <code>references</code>.</p>"},{"location":"tutorials/","title":"Example of tuto wtf i don't know i to name","text":""},{"location":"tutorials/#overview","title":"Overview","text":"<p>Briefly describe what this component or feature does.</p>"},{"location":"tutorials/#prerequisites","title":"Prerequisites","text":"<p>List any prerequisites or dependencies required.</p>"},{"location":"tutorials/#usage","title":"Usage","text":"<p>Show how to use this component with code examples.</p> <pre><code># Example code\nfrom your_project import YourComponent\n\ncomponent = YourComponent(param1=value1)\nresult = component.process(data)\n</code></pre>"},{"location":"tutorials/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>param1</code> str <code>None</code> Description of param1 <code>param2</code> int <code>42</code> Description of param2"},{"location":"tutorials/#return-values","title":"Return Values","text":"<p>Describe what is returned and in what format.</p>"},{"location":"tutorials/#examples","title":"Examples","text":"<p>Provide 1-2 complete examples with expected outputs.</p>"},{"location":"tutorials/#common-issues","title":"Common Issues","text":"<p>List common problems and their solutions.</p>"},{"location":"tutorials/#see-also","title":"See Also","text":"<p>Link to related documentation.</p>"},{"location":"tutorials/runpod_serverless/","title":"Guide exhaustif de d\u00e9ploiement RunPod","text":"<p>Ce guide documente toutes les options pratiques pour d\u00e9ployer vos services BurkimbIA sur RunPod, qu'il s'agisse de simples tests manuels depuis le tableau de bord ou d'un pipeline complet Infrastructure-as-Code (IaC). Nous nous concentrons ici sur le service de traduction fran\u00e7ais &lt;-&gt; moor\u00e9 afin de garder l'exemple concret et reproductible.</p>"},{"location":"tutorials/runpod_serverless/#1-comprendre-le-contexte-runpod","title":"1. Comprendre le contexte RunPod","text":"<p>RunPod propose trois approches compl\u00e9mentaires : - Console RunPod : interface graphique \"Deploy\" et \"Create Endpoint\" pour un d\u00e9marrage rapide. - GitHub + RunPod : RunPod peut consommer un repository GitHub et d\u00e9clencher vos builds (RunPod Deploys). - API RunPod : appels REST permettant d'automatiser totalement la cr\u00e9ation de templates et d'endpoints (IaC).</p> <p>Vous n'\u00eates pas oblig\u00e9 d'utiliser notre code Python interne (<code>RunPodClient</code>) ni nos workflows GitHub. Ce guide vous d\u00e9crit chaque m\u00e9thode pas \u00e0 pas et liste les probl\u00e8mes fr\u00e9quents rencontr\u00e9s lors de notre retour d'exp\u00e9rience.</p>"},{"location":"tutorials/runpod_serverless/#2-prerequis-generaux","title":"2. Pr\u00e9requis g\u00e9n\u00e9raux","text":"<ul> <li>Image Docker construite et pouss\u00e9e sur Docker Hub ou un registre priv\u00e9 (taille optimis\u00e9e &lt; 5 GB si possible). Crucial : int\u00e9grer le mod\u00e8le (ici NLLB) dans l'image pour \u00e9viter les t\u00e9l\u00e9chargements \u00e0 chaque cold start.</li> <li>Variables sensibles : <code>RUNPOD_API_KEY</code>, <code>HF_TOKEN</code> (pour acc\u00e8s mod\u00e8les priv\u00e9s), identifiants \u00e9ventuels vers stockage S3/MinIO.</li> <li>Fichiers BurkimbIA : <code>runpod-config.json</code>, <code>deploy_runpod.py</code>, <code>services/translation/test_input.json</code>.</li> <li>Documentation co\u00fbts : voir <code>PRICING_REFERENCES.md</code> et la liste officielle des GPU RunPod pour choisir le profil adapt\u00e9 \u00e0 votre budget.</li> <li>Tests locaux : ex\u00e9cuter <code>docker run ...</code> en local avec les variables d'environnement cl\u00e9s pour valider d\u00e9marrage + healthcheck.</li> </ul>"},{"location":"tutorials/runpod_serverless/#3-problemes-recurrents-a-surveiller","title":"3. Probl\u00e8mes r\u00e9currents \u00e0 surveiller","text":"<ol> <li>Jetons Hugging Face absents : impossible de t\u00e9l\u00e9charger les poids priv\u00e9s -&gt; l'init \u00e9choue ou boucle.</li> <li>Timeouts par d\u00e9faut trop courts : RunPod coupe le worker si <code>RUNPOD_INIT_TIMEOUT</code> ou <code>executionTimeout</code> sont insuffisants.</li> <li>Images surdimensionn\u00e9es : &gt;7 GB = cold starts tr\u00e8s longs et co\u00fbts r\u00e9seau \u00e9lev\u00e9s.</li> <li>Mod\u00e8les t\u00e9l\u00e9charg\u00e9s \u00e0 chaque d\u00e9marrage : sans int\u00e9grer les poids dans l'image Docker, chaque cold start re-t\u00e9l\u00e9charge le mod\u00e8le (NLLB = ~2.5 GB), causant des latences de 60-120s.</li> <li>Secrets non inject\u00e9s : diff\u00e9rence entre votre <code>.env</code> local et l'environnement c\u00f4t\u00e9 RunPod.</li> <li>Healthcheck/heartbeat non impl\u00e9ment\u00e9 : RunPod pense que le container est down.</li> <li>Ping storm : sans <code>idleTimeout</code> court, un endpoint re\u00e7oit des pings fr\u00e9quents et consomme du GPU inutilement.</li> </ol> <p>Nous d\u00e9taillons pour chaque approche comment \u00e9viter ces points.</p>"},{"location":"tutorials/runpod_serverless/#4-option-a-deploiement-100-manuel-depuis-la-console-runpod","title":"4. Option A \u2014 D\u00e9ploiement 100% manuel depuis la console RunPod","text":"<p>Id\u00e9al pour tester rapidement un prototype sans pipeline.</p> <ol> <li>Cr\u00e9er un compte RunPod et r\u00e9cup\u00e9rer votre cl\u00e9 API (Account &gt; API Keys).</li> <li>Acc\u00e9der au dashboard &gt; Serverless &gt; Deploy Endpoint.</li> <li>Choisir bring your own container :</li> <li>Image : <code>docker.io/sawalle/translation-service:latest</code> (ou votre tag personnalis\u00e9).</li> <li>Entrypoint et CMD : laissez ceux du Dockerfile.</li> <li>GPU : s\u00e9lectionner NVIDIA RTX A6000 ou NVIDIA L40 (bon \u00e9quilibre co\u00fbt/perf pour la traduction NLLB).</li> <li>Configurer l'onglet environment :</li> <li>Ajouter <code>HF_TOKEN</code>, <code>MODEL_PATH</code>, <code>LANGUAGE_CODE</code>, etc.</li> <li>D\u00e9finir <code>RUNPOD_INIT_TIMEOUT=800</code> et <code>RUNPOD_STOP_TIMEOUT=10</code>.</li> <li>D\u00e9finir les timeouts/scaling :<ul> <li><code>idleTimeoutSeconds = 5</code> pour \u00e9viter les pings co\u00fbteux.</li> <li><code>minWorkers = 0</code>, <code>maxWorkers = 1</code> (budget serr\u00e9 BurkimbIA).</li> </ul> </li> <li><code>executionTimeoutSeconds = 90</code> pour les requ\u00eates de traduction volumineuses.</li> <li>Sauvegarder le template puis cliquer sur <code>Deploy</code>.</li> <li>Tester via la console :</li> <li><code>Serverless</code> &gt; votre endpoint &gt; <code>Run</code>.</li> <li>Coller <code>services/translation/test_input.json</code>.</li> <li>Consulter les logs (<code>Logs</code> tab) pour v\u00e9rifier chargement du mod\u00e8le, latence, \u00e9ventuelles erreurs.</li> </ol> <p>Diagnostic manuel : - Erreur <code>ModuleNotFoundError</code> -&gt; v\u00e9rifier requirements Docker. - Erreur 401 HF -&gt; v\u00e9rifier <code>HF_TOKEN</code> et les permissions HF (read access). - Timeout -&gt; augmenter <code>executionTimeoutSeconds</code> ou optimiser mod\u00e8le.</p>"},{"location":"tutorials/runpod_serverless/#5-option-b-deploiement-via-un-repository-github-runpod-deploys","title":"5. Option B \u2014 D\u00e9ploiement via un repository GitHub + RunPod Deploys","text":"<p>Adapt\u00e9 si vous souhaitez que RunPod construise automatiquement votre image depuis GitHub et d\u00e9clenche les mises \u00e0 jour \u00e0 chaque commit.</p>"},{"location":"tutorials/runpod_serverless/#51-creer-votre-repository-etape-par-etape","title":"5.1 Cr\u00e9er votre repository \u00e9tape par \u00e9tape","text":"<p>Hypoth\u00e8se : vous partez de z\u00e9ro et vous voulez une structure simple, reproductible, focalis\u00e9e sur la traduction.</p> <p>\u00c9tape 1 \u2014 Initialiser le repository</p> <pre><code>mkdir burkimbia-runpod-translation &amp;&amp; cd burkimbia-runpod-translation\ngit init\n</code></pre> <p>\u00c9tape 2 \u2014 Poser l'arborescence minimale</p> <pre><code>.\n\u251c\u2500\u2500 deployment/\n\u2502   \u251c\u2500\u2500 deploy_runpod.py\n\u2502   \u2514\u2500\u2500 runpod-config.json\n\u251c\u2500\u2500 services/\n\u2502   \u2514\u2500\u2500 translation/\n\u2502       \u251c\u2500\u2500 Dockerfile\n\u2502       \u251c\u2500\u2500 src/\n\u2502       \u2502   \u2514\u2500\u2500 handler.py\n\u2502       \u251c\u2500\u2500 requirements.txt\n\u2502       \u2514\u2500\u2500 test_input.json\n\u251c\u2500\u2500 scripts/\n\u2502   \u2514\u2500\u2500 runpod_client.py\n\u251c\u2500\u2500 workflows/\n\u2502   \u2514\u2500\u2500 build-and-push.yml\n\u251c\u2500\u2500 requirements.txt\n\u2514\u2500\u2500 README.md\n</code></pre> <ul> <li>Le service de traduction poss\u00e8de son <code>Dockerfile</code>, un <code>src/handler.py</code> (RunPod handler) et un <code>test_input.json</code> pr\u00eat \u00e0 l'emploi.</li> <li>Les \u00e9l\u00e9ments d'infrastructure (<code>deploy_runpod.py</code>, <code>runpod-config.json</code>) vivent dans <code>deployment/</code>.</li> <li>Les ressources CI/CD sont regroup\u00e9es dans <code>workflows/</code> afin de pouvoir \u00eatre copi\u00e9es dans <code>.github/workflows/</code> le moment venu.</li> </ul> <p>\u00c9tape 3 \u2014 Fournir un payload de test</p> <p>Contenu recommand\u00e9 pour <code>services/translation/test_input.json</code> :</p> <pre><code>{\n   \"input\": {\n      \"text\": [\"Bonjour\"],\n      \"src_lang\": \"fra_Latn\",\n      \"tgt_lang\": \"moo_Latn\"\n   }\n}\n</code></pre> <p>\u00c9tape 4 \u2014 Cr\u00e9er un Dockerfile optimis\u00e9</p> <p>Le secret pour \u00e9viter les cold starts longs (&gt;60s) est d'int\u00e9grer le mod\u00e8le NLLB directement dans l'image Docker plut\u00f4t que de le t\u00e9l\u00e9charger \u00e0 chaque d\u00e9marrage. Voici un exemple pour <code>services/translation/Dockerfile</code> bas\u00e9 sur le Dockerfile BurkimbIA en production :</p> <pre><code>FROM pytorch/pytorch:2.7.1-cuda11.8-cudnn9-runtime\n\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    build-essential \\\n    gcc \\\n    g++ \\\n    curl \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\nWORKDIR /app\n\n# Install Python dependencies first (better caching)\nCOPY requirements.txt .\nRUN pip install --no-cache-dir --upgrade pip &amp;&amp; \\\n    pip install -r requirements.txt\n\n# Setup Hugging Face authentication using official CLI method\nARG HF_TOKEN\nRUN huggingface-cli login --token ${HF_TOKEN}\n\n# Download models using huggingface-cli (BEST PRACTICE for RunPod)\n# This embeds the models in the Docker image for fastest loading\nRUN huggingface-cli download burkimbia/nllb_600M_v0.0.2 --local-dir /app/models/BIA-NLLB-600M-5E &amp;&amp; \\\n    # Clean up any potential .git folders to reduce image size\n    find /app/models -name \".git\" -type d -exec rm -rf {} + 2&gt;/dev/null || true\n\n# Copy application code\nCOPY src/ ./src/\nCOPY test_input.json ./test_input.json\n\n# Create non-root user for security\nRUN useradd --create-home --shell /bin/bash app &amp;&amp; \\\n    chown -R app:app /app\n\nUSER app\n\n# Set environment variables\nENV MODEL_PATH=\"/app/models/BIA-NLLB-600M-5E\"\nENV PYTHONPATH=\"/app\"\n\nCMD [\"python\", \"-u\", \"src/handler.py\"]\n</code></pre> <p>Gain de performance : Avec <code>huggingface-cli download</code> pendant le build, le mod\u00e8le NLLB (600M) est int\u00e9gr\u00e9 dans l'image. Cold start passe de 60-120s \u00e0 5-10s !</p> <p>\u00c9tape 5 \u2014 Cr\u00e9er un handler.py avec chargement local</p> <p>Contenu recommand\u00e9 pour <code>services/translation/src/handler.py</code> :</p> <pre><code>import os\nimport logging\nfrom typing import Dict, List, Any\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nimport runpod\n\n# Configuration des logs\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Variables globales pour le mod\u00e8le (charg\u00e9 une seule fois)\nMODEL = None\nTOKENIZER = None\nMODEL_PATH = os.getenv(\"MODEL_PATH\", \"/app/models/BIA-NLLB-600M-5E\")\n\ndef load_model():\n    \"\"\"Charge le mod\u00e8le NLLB depuis le stockage local.\"\"\"\n    global MODEL, TOKENIZER\n    if MODEL is None:\n        logger.info(f\"Loading NLLB model from {MODEL_PATH}\")\n        try:\n            TOKENIZER = AutoTokenizer.from_pretrained(MODEL_PATH)\n            MODEL = AutoModelForSeq2SeqLM.from_pretrained(MODEL_PATH)\n            logger.info(\"Model loaded successfully from local storage\")\n        except Exception as e:\n            logger.error(f\"Failed to load model: {str(e)}\")\n            raise\n    return MODEL, TOKENIZER\n\ndef translate_text(texts: List[str], src_lang: str, tgt_lang: str) -&gt; List[str]:\n    \"\"\"Traduit une liste de textes.\"\"\"\n    model, tokenizer = load_model()\n\n    # Pr\u00e9paration des entr\u00e9es\n    tokenizer.src_lang = src_lang\n    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n\n    # G\u00e9n\u00e9ration\n    generated_tokens = model.generate(\n        **inputs,\n        forced_bos_token_id=tokenizer.lang_code_to_id[tgt_lang],\n        max_length=512,\n        num_beams=5,\n        early_stopping=True\n    )\n\n    # D\u00e9codage\n    translations = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n    return translations\n\ndef handler(event: Dict[str, Any]) -&gt; Dict[str, Any]:\n    \"\"\"Handler RunPod pour la traduction.\"\"\"\n    try:\n        input_data = event.get(\"input\", {})\n        texts = input_data.get(\"text\", [])\n        src_lang = input_data.get(\"src_lang\", \"fra_Latn\")\n        tgt_lang = input_data.get(\"tgt_lang\", \"moo_Latn\")\n\n        if not texts:\n            return {\"error\": \"No text provided\"}\n\n        logger.info(f\"Translating {len(texts)} texts: {src_lang} -&gt; {tgt_lang}\")\n        translations = translate_text(texts, src_lang, tgt_lang)\n\n        return {\n            \"translations\": translations,\n            \"source_language\": src_lang,\n            \"target_language\": tgt_lang,\n            \"model_path\": MODEL_PATH\n        }\n\n    except Exception as e:\n        logger.error(f\"Translation failed: {str(e)}\")\n        return {\"error\": str(e)}\n\nif __name__ == \"__main__\":\n    # Pr\u00e9chargement du mod\u00e8le au d\u00e9marrage\n    load_model()\n    logger.info(\"Translation service ready\")\n\n    # D\u00e9marrer le serveur RunPod\n    runpod.serverless.start({\"handler\": handler})\n</code></pre> <p>\u00c9tape 6 \u2014 Fichier requirements.txt pour le service</p> <p>Contenu pour <code>services/translation/requirements.txt</code> :</p> <pre><code>torch&gt;=2.0.0\ntransformers&gt;=4.30.0\nsentencepiece&gt;=0.1.99\nrunpod&gt;=1.6.0\nhuggingface-hub&gt;=0.17.0\n</code></pre> <p>\u00c9tape 7 \u2014 D\u00e9finir la configuration RunPod</p> <p>Placez le fichier suivant dans <code>deployment/runpod-config.json</code>.</p>"},{"location":"tutorials/runpod_serverless/#511-contenu-de-deploymentrunpod-configjson","title":"5.1.1 Contenu de <code>deployment/runpod-config.json</code>","text":"<pre><code>{\n   \"translation_endpoint\": {\n      \"endpoint_name\": \"burkimbia-translation-service\",\n      \"description\": \"Service de traduction fra &lt;-&gt; moor\u00e9\",\n      \"image\": \"docker.io/YOUR_DOCKER_USERNAME/translation-service:latest\",\n      \"gpu_types\": [\"NVIDIA RTX A4000\", \"NVIDIA RTX A6000\"],\n      \"idle_timeout\": 5,\n      \"execution_timeout\": 90,\n      \"scaling\": {\n         \"min_workers\": 0,\n         \"max_workers\": 1\n      },\n      \"env\": [\n         {\"key\": \"SERVICE_NAME\", \"value\": \"translation\"},\n         {\"key\": \"RUNPOD_INIT_TIMEOUT\", \"value\": \"800\"},\n         {\"key\": \"HF_TOKEN\", \"value\": \"${HF_TOKEN}\"},\n         {\"key\": \"MODEL_PATH\", \"value\": \"/app/models/BIA-NLLB-600M-5E\"}\n      ],\n      \"test_payload\": \"services/translation/test_input.json\"\n   }\n}\n</code></pre>"},{"location":"tutorials/runpod_serverless/#512-contenu-de-deploymentdeploy_runpodpy","title":"5.1.2 Contenu de <code>deployment/deploy_runpod.py</code>","text":"<pre><code>import argparse\nimport json\nimport pathlib\nimport sys\nfrom typing import Any, Dict\n\nimport requests\n\n\nRUNPOD_API_BASE = \"https://api.runpod.ai/v2\"\n\n\ndef load_config(path: pathlib.Path) -&gt; Dict[str, Any]:\n   try:\n      return json.loads(path.read_text(encoding=\"utf-8\"))\n   except FileNotFoundError as exc:\n      raise SystemExit(f\"Config file not found: {path}\") from exc\n\n\ndef build_headers(api_key: str) -&gt; Dict[str, str]:\n   return {\n      \"Authorization\": api_key,\n      \"Content-Type\": \"application/json\"\n   }\n\n\ndef upsert_template(api_key: str, payload: Dict[str, Any]) -&gt; str:\n   response = requests.post(\n      f\"{RUNPOD_API_BASE}/endpoints\",\n      headers=build_headers(api_key),\n      json=payload,\n      timeout=30\n   )\n   response.raise_for_status()\n   data = response.json()\n   template_id = data.get(\"id\") or data.get(\"endpointId\")\n   if not template_id:\n      raise SystemExit(f\"Unexpected response: {data}\")\n   return template_id\n\n\ndef deploy_endpoint(api_key: str, template_id: str) -&gt; Dict[str, Any]:\n   response = requests.post(\n      f\"{RUNPOD_API_BASE}/{template_id}/deploy\",\n      headers=build_headers(api_key),\n      json={},\n      timeout=30\n   )\n   response.raise_for_status()\n   return response.json()\n\n\ndef load_payload(raw_config: Dict[str, Any], service: str) -&gt; Dict[str, Any]:\n   block = raw_config.get(service)\n   if not block:\n      raise SystemExit(f\"Service '{service}' not found in config\")\n   payload = {\n      \"name\": block[\"endpoint_name\"],\n      \"description\": block.get(\"description\", \"\"),\n      \"imageName\": block[\"image\"],\n      \"gpuTypes\": block.get(\"gpu_types\", []),\n      \"idleTimeout\": block.get(\"idle_timeout\", 5),\n      \"restartPolicy\": \"OnFailure\",\n      \"scaleConfig\": {\n         \"min\": block[\"scaling\"][\"min_workers\"],\n         \"max\": block[\"scaling\"][\"max_workers\"],\n         \"batchSize\": 1\n      },\n      \"env\": block.get(\"env\", []),\n      \"workerConfig\": {\n         \"timeout\": block.get(\"execution_timeout\", 60)\n      }\n   }\n   return payload\n\n\ndef main() -&gt; None:\n   parser = argparse.ArgumentParser(description=\"Deploy RunPod endpoints from config\")\n   parser.add_argument(\"--config\", default=\"runpod-config.json\", help=\"Chemin fichier config\")\n   parser.add_argument(\"--service\", required=True, help=\"Identifiant du service \u00e0 d\u00e9ployer\")\n   parser.add_argument(\"--api-key\", default=None, help=\"RunPod API key (sinon RUNPOD_API_KEY)\")\n   parser.add_argument(\"--dry-run\", action=\"store_true\", help=\"Afficher la payload sans appeler RunPod\")\n   args = parser.parse_args()\n\n   api_key = args.api_key or pathlib.os.getenv(\"RUNPOD_API_KEY\")\n   if not api_key:\n      raise SystemExit(\"RunPod API key missing. Set RUNPOD_API_KEY or use --api-key\")\n\n   raw_config = load_config(pathlib.Path(args.config))\n   payload = load_payload(raw_config, args.service)\n\n   if args.dry_run:\n      print(json.dumps(payload, indent=2))\n      sys.exit(0)\n\n   template_id = upsert_template(api_key, payload)\n   result = deploy_endpoint(api_key, template_id)\n   print(f\"Endpoint deployed: {result}\")\n\n\nif __name__ == \"__main__\":\n   main()\n</code></pre>"},{"location":"tutorials/runpod_serverless/#513-contenu-de-scriptsrunpod_clientpy","title":"5.1.3 Contenu de <code>scripts/runpod_client.py</code>","text":"<pre><code>import time\nfrom typing import Any, Dict, Optional\n\nimport requests\n\n\nclass RunPodClient:\n   def __init__(self, api_token: str, base_url: str = \"https://api.runpod.ai/v2\") -&gt; None:\n      self.api_token = api_token\n      self.base_url = base_url\n\n   def _headers(self) -&gt; Dict[str, str]:\n      return {\n         \"Authorization\": self.api_token,\n         \"Content-Type\": \"application/json\"\n      }\n\n   def submit(self, endpoint_id: str, payload: Dict[str, Any]) -&gt; str:\n      response = requests.post(\n         f\"{self.base_url}/{endpoint_id}/run\",\n         headers=self._headers(),\n         json=payload,\n         timeout=30\n      )\n      response.raise_for_status()\n      data = response.json()\n      job_id = data.get(\"id\") or data.get(\"jobId\")\n      if not job_id:\n         raise RuntimeError(f\"Invalid submit response: {data}\")\n      return job_id\n\n   def poll(self, endpoint_id: str, job_id: str, timeout_s: int = 120, interval_s: int = 5) -&gt; Optional[Dict[str, Any]]:\n      deadline = time.time() + timeout_s\n      while time.time() &lt; deadline:\n         response = requests.get(\n            f\"{self.base_url}/{endpoint_id}/status/{job_id}\",\n            headers=self._headers(),\n            timeout=15\n         )\n         response.raise_for_status()\n         data = response.json()\n         status = data.get(\"status\")\n         if status in {\"COMPLETED\", \"FAILED\", \"CANCELLED\"}:\n            return data\n         time.sleep(interval_s)\n      return None\n\n   def run_and_wait(self, endpoint_id: str, payload: Dict[str, Any], timeout_s: int = 120) -&gt; Dict[str, Any]:\n      job_id = self.submit(endpoint_id, payload)\n      result = self.poll(endpoint_id, job_id, timeout_s=timeout_s)\n      if result is None:\n         raise TimeoutError(f\"Job {job_id} timed out\")\n      return result\n</code></pre>"},{"location":"tutorials/runpod_serverless/#514-contenu-de-requirementstxt","title":"5.1.4 Contenu de <code>requirements.txt</code>","text":"<pre><code>requests&gt;=2.32.0\n</code></pre> <p>Ajoutez les d\u00e9pendances translation (transformers, sentencepiece, sacrebleu, etc.) directement dans le Dockerfile pour garder ce fichier l\u00e9ger.</p> <p>Copiez ensuite <code>workflows/build-and-push.yml</code> vers <code>.github/workflows/build-and-push.yml</code> et utilisez ce contenu :</p> <pre><code>name: Build and push\non:\n   workflow_dispatch:\n   push:\n      branches: [ main ]\n\njobs:\n   build:\n      runs-on: ubuntu-latest\n      steps:\n         - uses: actions/checkout@v4\n         - name: Log in to Docker Hub\n            run: echo \"$DOCKER_PASSWORD\" | docker login -u \"$DOCKER_USERNAME\" --password-stdin\n            env:\n               DOCKER_PASSWORD: ${{ secrets.DOCKER_PASSWORD }}\n               DOCKER_USERNAME: ${{ secrets.DOCKER_USERNAME }}\n         - name: Build image\n            run: |\n               docker build \\\n                  --build-arg HF_TOKEN=${{ secrets.HF_TOKEN }} \\\n                  -t ${{ secrets.DOCKER_USERNAME }}/translation-service:latest \\\n                  services/translation/\n         - name: Push image\n            run: docker push ${{ secrets.DOCKER_USERNAME }}/translation-service:latest\n</code></pre> <p>Important : Passer le <code>HF_TOKEN</code> comme build argument permet de t\u00e9l\u00e9charger le mod\u00e8le pendant la construction de l'image, \u00e9liminant ainsi les cold starts longs.</p> <ol> <li>D\u00e9clarer les secrets GitHub suivants : <code>DOCKER_USERNAME</code>, <code>DOCKER_PASSWORD</code>, <code>HF_TOKEN</code>, <code>RUNPOD_API_KEY</code>.</li> </ol>"},{"location":"tutorials/runpod_serverless/#52-connecter-runpod-a-github","title":"5.2 Connecter RunPod \u00e0 GitHub","text":"<ol> <li>Dans RunPod, cliquer sur <code>Deploy</code> &gt; <code>Deploy from GitHub</code>.</li> <li>Autoriser RunPod \u00e0 acc\u00e9der \u00e0 votre organisation/repo.</li> <li>S\u00e9lectionner le repository, la branche et le dossier contenant le Dockerfile.</li> <li>Configurer le type de GPU, les variables d'environnement, timeouts et scaling directement dans l'assistant.</li> <li>Activer <code>Autodeploy on push</code> si souhait\u00e9.</li> </ol>"},{"location":"tutorials/runpod_serverless/#53-problemes-courants","title":"5.3 Probl\u00e8mes courants","text":"<ul> <li>RunPod ne voit pas votre Dockerfile -&gt; v\u00e9rifier <code>Context path</code>.</li> <li>Build \u00e9choue -&gt; taille de l'image ou d\u00e9pendances (utiliser base <code>runpod/base:...</code> si besoin).</li> <li>Secrets Docker manquants -&gt; configurer <code>Build secrets</code> dans RunPod.</li> </ul>"},{"location":"tutorials/runpod_serverless/#6-option-c-infrastructure-as-code-via-api-runpod","title":"6. Option C \u2014 Infrastructure as Code via API RunPod","text":"<p>Approche privil\u00e9gi\u00e9e par BurkimbIA pour industrialiser les d\u00e9ploiements et documenter la configuration.</p>"},{"location":"tutorials/runpod_serverless/#61-structure-des-fichiers","title":"6.1 Structure des fichiers","text":"<ul> <li><code>runpod-config.json</code> : r\u00e9f\u00e9rence centrale d\u00e9crivant l'endpoint de traduction.</li> <li><code>deploy_runpod.py</code> : script Python qui lit la config et appelle l'API RunPod.</li> <li><code>scripts/runpod_client.py</code> : client Python optionnel pour consommer l'endpoint d\u00e9ploy\u00e9.</li> </ul>"},{"location":"tutorials/runpod_serverless/#62-variables-denvironnement","title":"6.2 Variables d'environnement","text":"<pre><code>export RUNPOD_API_KEY=\"sk_live_xxxxxx\"\nexport HF_TOKEN=\"hf_xxxxxx\"\nexport RUNPOD_ACCOUNT_ID=\"your_account\"   # optionnel si multi-org\n</code></pre>"},{"location":"tutorials/runpod_serverless/#63-lancer-le-deploiement","title":"6.3 Lancer le d\u00e9ploiement","text":"<pre><code>python deployment/deploy_runpod.py \\\n   --service translation_endpoint \\\n   --config deployment/runpod-config.json\n</code></pre> <p>Options utiles : - <code>--dry-run</code> : affiche la payload envoy\u00e9e \u00e0 RunPod sans ex\u00e9cuter la requ\u00eate. - <code>--api-key</code> : passer la cl\u00e9 directement en argument (sinon variable d'environnement <code>RUNPOD_API_KEY</code>).</p>"},{"location":"tutorials/runpod_serverless/#64-exemple-dappel-api-brut","title":"6.4 Exemple d'appel API brut","text":"<p>Pour cr\u00e9er un template via <code>curl</code> :</p> <pre><code>curl -X POST https://api.runpod.ai/v2/endpoints \\\n   -H \"Authorization: ${RUNPOD_API_KEY}\" \\\n   -H \"Content-Type: application/json\" \\\n   -d @payload.json\n</code></pre> <p><code>payload.json</code> peut \u00eatre g\u00e9n\u00e9r\u00e9 depuis <code>runpod-config.json</code>. Exemple minimal :</p> <pre><code>{\n   \"name\": \"burkimbia-translation-service\",\n   \"imageName\": \"docker.io/sawalle/translation-service:latest\",\n   \"gpuTypes\": [\"NVIDIA RTX A4000\"],\n   \"minWorkers\": 0,\n   \"maxWorkers\": 1,\n   \"idleTimeout\": 5,\n   \"env\": [\n      {\"key\": \"HF_TOKEN\", \"value\": \"${HF_TOKEN}\"},\n      {\"key\": \"RUNPOD_INIT_TIMEOUT\", \"value\": \"800\"}\n   ],\n   \"ports\": [],\n   \"volumeMounts\": []\n}\n</code></pre>"},{"location":"tutorials/runpod_serverless/#65-avantages-de-liac","title":"6.5 Avantages de l'IaC","text":"<ul> <li>Historique Git complet des changements d'infrastructure.</li> <li>Reproductibilit\u00e9 (staging vs production).</li> <li>Possibilit\u00e9 d'int\u00e9grer des tests (ex. d\u00e9clencher une requ\u00eate de sant\u00e9 apr\u00e8s d\u00e9ploiement).</li> <li>Compatible avec d'autres orchestrateurs (Terraform via provider HTTP, Pulumi via Python).</li> </ul>"},{"location":"tutorials/runpod_serverless/#7-diagnostic-et-support","title":"7. Diagnostic et support","text":""},{"location":"tutorials/runpod_serverless/#71-logs-et-metriques","title":"7.1 Logs et m\u00e9triques","text":"<ul> <li>RunPod Console &gt; Endpoint &gt; <code>Logs</code> pour les journaux temps r\u00e9el.</li> <li><code>Metrics</code> fournit le nombre de requ\u00eates, latence, cold starts.</li> <li>Ajouter des logs structur\u00e9s JSON c\u00f4t\u00e9 application pour tracer le co\u00fbt par requ\u00eate (cf. <code>monitoring.py</code>).</li> </ul>"},{"location":"tutorials/runpod_serverless/#72-tests-dinference","title":"7.2 Tests d'inf\u00e9rence","text":"<p>Si vous n'utilisez pas <code>RunPodClient</code>, voici un <code>curl</code> g\u00e9n\u00e9rique :</p> <pre><code>curl -X POST https://api.runpod.ai/v2/&lt;ENDPOINT_ID&gt;/run \\\n   -H \"Authorization: ${RUNPOD_API_KEY}\" \\\n   -H \"Content-Type: application/json\" \\\n   -d @services/translation/test_input.json\n</code></pre> <p>R\u00e9ponse : un <code>jobId</code> \u00e0 poller.</p> <pre><code>curl -X GET https://api.runpod.ai/v2/&lt;ENDPOINT_ID&gt;/status/&lt;JOB_ID&gt; \\\n   -H \"Authorization: ${RUNPOD_API_KEY}\"\n</code></pre>"},{"location":"tutorials/runpod_serverless/#73-erreurs-typiques-et-remedes","title":"7.3 Erreurs typiques et rem\u00e8des","text":"Symptom Cause probable Correction 404 template Mauvais endpoint ID V\u00e9rifier <code>runpod_deployed_endpoints.json</code> Status <code>FAILED_JOB</code> Exception Python Lire logs, v\u00e9rifier d\u00e9pendances Load &gt; 90s Image trop lourde Utiliser base + quantification 4-bit Load &gt; 90s (mod\u00e8le) T\u00e9l\u00e9chargement NLLB \u00e0 chaque start Int\u00e9grer mod\u00e8le dans Dockerfile avec ARG HF_TOKEN GPU indisponible Rupture de stock RunPod Ajouter fallback <code>gpuTypes</code> (A4000, A6000)"},{"location":"tutorials/runpod_serverless/#8-monitoring-couts-et-gouvernance","title":"8. Monitoring, co\u00fbts et gouvernance","text":"<ul> <li>Co\u00fbts r\u00e9els BurkimbIA : voir <code>PRICING_REFERENCES.md</code> (A6000 serverless = $8.67/mois pour 3000 requ\u00eates de traduction fra-&gt;moor\u00e9).</li> <li>Activer les alertes budg\u00e9taires RunPod (<code>Settings</code> &gt; <code>Billing alerts</code>).</li> <li>Exporter les logs vers un stockage externe pour audit (S3/Backblaze).</li> <li>Mettre en place un <code>cron</code> qui arr\u00eate les endpoints inactifs (<code>/endpoint/&lt;id&gt;/pause</code>).</li> </ul>"},{"location":"tutorials/runpod_serverless/#9-roadmap-damelioration-continue","title":"9. Roadmap d'am\u00e9lioration continue","text":"<ol> <li>Automatiser les tests d'acceptation : script qui d\u00e9ploie, envoie une requ\u00eate, v\u00e9rifie la qualit\u00e9 de traduction.</li> <li>Ajout Terraform/Pulumi : g\u00e9n\u00e9rer les payloads RunPod depuis IaC mainstream.</li> <li>Gestion multi-mod\u00e8les : templating Jinja dans <code>runpod-config.json</code> pour d\u00e9cliner les GPU selon la paire de langues.</li> <li>M\u00e9canisme de rollback : conserver les <code>templateId</code> pr\u00e9c\u00e9dents pour repasser \u00e0 une version stable.</li> </ol>"},{"location":"tutorials/runpod_serverless/#10-checklist-rapide-a-coller-dans-vos-pr","title":"10. Checklist rapide (\u00e0 coller dans vos PR)","text":"<ul> <li> Docker image construite localement et scann\u00e9e (<code>docker scan</code>).</li> <li> Variables d'environnement list\u00e9es et document\u00e9es.</li> <li> <code>runpod-config.json</code> mis \u00e0 jour et valid\u00e9 avec <code>jsonschema</code>.</li> <li> Tests d'inf\u00e9rence ex\u00e9cut\u00e9s (curl + script Python).</li> <li> Cost impact \u00e9valu\u00e9 (GPU, temps moyen, budget BurkimbIA).</li> <li> Documentation interne mise \u00e0 jour (<code>RUNPOD_DEPLOYMENT_GUIDE.md</code>, <code>PRICING_REFERENCES.md</code>).</li> </ul> <p>Avec ces trois options (console, GitHub, API), vous pouvez d\u00e9marrer rapidement et \u00e9voluer vers une approche IaC robuste en fonction de votre maturit\u00e9 et de vos contraintes.</p>"}]}